{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exogenous Data Preprocessing Notebook**\n",
    "## In this notebook we will preprocess our exogenous data, first creating our lag windows and the custom features that come from expanding the resulting view.  Then we will do any necessary cleaning such as reviewing outliers and missing values in our expanded data, imputing as needed so we have a clean dataset going into the next phase of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by bringing in the libraries and logic necessary for reading in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's read in our data that we need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            interest_rates_10yr         gold  copper    platinum  silver  \\\n",
      "Date                                                                       \n",
      "2019-01-01                2.661  1281.000000  2.6250  799.099976  15.542   \n",
      "2019-01-02                2.661  1281.000000  2.6250  799.099976  15.542   \n",
      "2019-01-03                2.554  1291.800049  2.5705  794.500000  15.706   \n",
      "2019-01-04                2.659  1282.699951  2.6515  822.000000  15.695   \n",
      "2019-01-07                2.682  1286.800049  2.6410  818.400024  15.669   \n",
      "\n",
      "            crude_oil  natural_gas    corn   wheat  volatility_index  \\\n",
      "Date                                                                   \n",
      "2019-01-01  46.540001        2.958  375.75  506.75         23.219999   \n",
      "2019-01-02  46.540001        2.958  375.75  506.75         23.219999   \n",
      "2019-01-03  47.090000        2.945  379.75  513.75         25.450001   \n",
      "2019-01-04  47.959999        3.044  383.00  517.00         21.379999   \n",
      "2019-01-07  48.520000        2.944  382.25  516.75         21.400000   \n",
      "\n",
      "            exchange_rate_usd_eur  exchange_rate_usd_jpy        sp500  \\\n",
      "Date                                                                    \n",
      "2019-01-01               1.149306             109.629997  2510.030029   \n",
      "2019-01-02               1.146171             109.667999  2510.030029   \n",
      "2019-01-03               1.131811             107.441002  2447.889893   \n",
      "2019-01-04               1.139108             107.807999  2531.939941   \n",
      "2019-01-07               1.141044             108.522003  2549.689941   \n",
      "\n",
      "             nasdaq_100  dow_jones_industrial_average  \\\n",
      "Date                                                    \n",
      "2019-01-01  6360.870117                  23346.240234   \n",
      "2019-01-02  6360.870117                  23346.240234   \n",
      "2019-01-03  6147.129883                  22686.220703   \n",
      "2019-01-04  6422.669922                  23433.160156   \n",
      "2019-01-07  6488.250000                  23531.349609   \n",
      "\n",
      "            consumer_confidence_index  vanguard_total_world_stock_etf  \\\n",
      "Date                                                                    \n",
      "2019-01-01                 105.949997                       65.379997   \n",
      "2019-01-02                 105.949997                       65.379997   \n",
      "2019-01-03                 106.480003                       64.360001   \n",
      "2019-01-04                 108.070000                       66.449997   \n",
      "2019-01-07                 108.599998                       66.860001   \n",
      "\n",
      "            us_treasury_bond_etf  \n",
      "Date                              \n",
      "2019-01-01             24.850000  \n",
      "2019-01-02             24.850000  \n",
      "2019-01-03             24.990000  \n",
      "2019-01-04             24.860001  \n",
      "2019-01-07             24.809999  \n",
      "(1467, 18)\n"
     ]
    }
   ],
   "source": [
    "# Now let's access the main core_stock_data.csv file\n",
    "csv_path = os.path.join(project_root, 'data', 'exogenous_data.csv')\n",
    "exo_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "print(exo_data.head())\n",
    "print(exo_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step I want to undertake is handling and imputing any missing features.  I want to do this first so that when we create our lag windows and resulting custom features the new values will be calculated properly, and it will be easier to address our resulting outliers.  We will look at the same process that we used for our core stock data, a combination of ffill, bfill and linear interpolation for max efficiency of our missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_rates_10yr               0\n",
      "gold                              0\n",
      "copper                            0\n",
      "platinum                          0\n",
      "silver                            0\n",
      "crude_oil                         0\n",
      "natural_gas                       0\n",
      "corn                              0\n",
      "wheat                             0\n",
      "volatility_index                  0\n",
      "exchange_rate_usd_eur             0\n",
      "exchange_rate_usd_jpy             0\n",
      "sp500                             0\n",
      "nasdaq_100                        0\n",
      "dow_jones_industrial_average      0\n",
      "consumer_confidence_index         0\n",
      "vanguard_total_world_stock_etf    0\n",
      "us_treasury_bond_etf              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def fill_missing_vals(df):\n",
    "    df.ffill(inplace = True)\n",
    "    df.bfill(inplace = True)\n",
    "    df.interpolate(method = 'linear', inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "exo_data = exo_data.copy()\n",
    "\n",
    "for col in exo_data.columns:\n",
    "    if exo_data[col].isnull().any():\n",
    "        exo_data.loc[:, col] = fill_missing_vals(exo_data[[col]])[col]\n",
    "\n",
    "\n",
    "\n",
    "print(exo_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks great, now let's create the lag windows and custom features for our secondary stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1467, 180)\n",
      "            interest_rates_10yr         gold  copper    platinum  silver  \\\n",
      "Date                                                                       \n",
      "2019-01-01                2.661  1281.000000  2.6250  799.099976  15.542   \n",
      "2019-01-02                2.661  1281.000000  2.6250  799.099976  15.542   \n",
      "2019-01-03                2.554  1291.800049  2.5705  794.500000  15.706   \n",
      "2019-01-04                2.659  1282.699951  2.6515  822.000000  15.695   \n",
      "2019-01-07                2.682  1286.800049  2.6410  818.400024  15.669   \n",
      "\n",
      "            crude_oil  natural_gas    corn   wheat  volatility_index  ...  \\\n",
      "Date                                                                  ...   \n",
      "2019-01-01  46.540001        2.958  375.75  506.75         23.219999  ...   \n",
      "2019-01-02  46.540001        2.958  375.75  506.75         23.219999  ...   \n",
      "2019-01-03  47.090000        2.945  379.75  513.75         25.450001  ...   \n",
      "2019-01-04  47.959999        3.044  383.00  517.00         21.379999  ...   \n",
      "2019-01-07  48.520000        2.944  382.25  516.75         21.400000  ...   \n",
      "\n",
      "            vanguard_total_world_stock_etf_Diff  us_treasury_bond_etf_Lag_1  \\\n",
      "Date                                                                          \n",
      "2019-01-01                                  NaN                         NaN   \n",
      "2019-01-02                             0.000000                   24.850000   \n",
      "2019-01-03                            -1.019997                   24.850000   \n",
      "2019-01-04                             2.089996                   24.990000   \n",
      "2019-01-07                             0.410004                   24.860001   \n",
      "\n",
      "            us_treasury_bond_etf_Lag_2  us_treasury_bond_etf_Lag_3  \\\n",
      "Date                                                                 \n",
      "2019-01-01                         NaN                         NaN   \n",
      "2019-01-02                         NaN                         NaN   \n",
      "2019-01-03                       24.85                         NaN   \n",
      "2019-01-04                       24.85                       24.85   \n",
      "2019-01-07                       24.99                       24.85   \n",
      "\n",
      "            us_treasury_bond_etf_MA_7  us_treasury_bond_etf_MA_30  \\\n",
      "Date                                                                \n",
      "2019-01-01                        NaN                         NaN   \n",
      "2019-01-02                        NaN                         NaN   \n",
      "2019-01-03                        NaN                         NaN   \n",
      "2019-01-04                        NaN                         NaN   \n",
      "2019-01-07                        NaN                         NaN   \n",
      "\n",
      "            us_treasury_bond_etf_Std_7  us_treasury_bond_etf_Std_30  \\\n",
      "Date                                                                  \n",
      "2019-01-01                         NaN                          NaN   \n",
      "2019-01-02                         NaN                          NaN   \n",
      "2019-01-03                         NaN                          NaN   \n",
      "2019-01-04                         NaN                          NaN   \n",
      "2019-01-07                         NaN                          NaN   \n",
      "\n",
      "            us_treasury_bond_etf_RoC  us_treasury_bond_etf_Diff  \n",
      "Date                                                             \n",
      "2019-01-01                       NaN                        NaN  \n",
      "2019-01-02                  0.000000                   0.000000  \n",
      "2019-01-03                  0.563378                   0.139999  \n",
      "2019-01-04                 -0.520205                  -0.129999  \n",
      "2019-01-07                 -0.201131                  -0.050001  \n",
      "\n",
      "[5 rows x 180 columns]\n",
      "interest_rates_10yr             0\n",
      "gold                            0\n",
      "copper                          0\n",
      "platinum                        0\n",
      "silver                          0\n",
      "                               ..\n",
      "us_treasury_bond_etf_MA_30     29\n",
      "us_treasury_bond_etf_Std_7      6\n",
      "us_treasury_bond_etf_Std_30    29\n",
      "us_treasury_bond_etf_RoC        1\n",
      "us_treasury_bond_etf_Diff       1\n",
      "Length: 180, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_47320\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n"
     ]
    }
   ],
   "source": [
    "# We will need to create a temp variable since we are making so many custom features, it will otherwise create a PerformanceWarning since when we execute the cell on our main df its too much at once.\n",
    "# By storing all the custom features to a temp variable and then concatenating them in a separate step it takes the pressure off in the execute phase.\n",
    "new_temp_columns = pd.DataFrame(index = exo_data.index)\n",
    "\n",
    "# Now let's create a variable to house our features we want to create lag features for.\n",
    "features_to_lag = ['interest_rates_10yr', 'gold', 'copper', 'platinum', 'silver', 'crude_oil',\n",
    "                    'natural_gas', 'corn', 'wheat', 'volatility_index', 'exchange_rate_usd_eur',\n",
    "                    'exchange_rate_usd_jpy', 'sp500', 'nasdaq_100', 'dow_jones_industrial_average',\n",
    "                    'consumer_confidence_index', 'vanguard_total_world_stock_etf', 'us_treasury_bond_etf']\n",
    "\n",
    "# Now we apply the lag windows with a small 3-day window size.\n",
    "for feature in features_to_lag:\n",
    "    for lag in range(1,4):\n",
    "        new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
    "\n",
    "# Great, now from these new lag-based features let's expand on it further by creating some custom features using our existing ones.\n",
    "# Moving Averages\n",
    "    new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
    "    new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
    "    # Volatility\n",
    "    new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
    "    new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
    "    # Rate of Change (Momentum)\n",
    "    new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
    "    # Difference Features\n",
    "    new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
    "\n",
    "# Now we can concatenate onto our original core_stock_data.\n",
    "exo_data = pd.concat([exo_data, new_temp_columns], axis = 1)\n",
    "\n",
    "print(exo_data.shape)\n",
    "print(exo_data.head())\n",
    "\n",
    "print(exo_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It looks like through the creation of all of our new features there are just a few missing values that slipped through.  Let's re-run our imputation logic to take care of them.  Also since I see 0s in our .head() printout, we will address those in our function below as leaving those will cause issues down the road with our outlier calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled:\n",
      "interest_rates_10yr            0\n",
      "gold                           0\n",
      "copper                         0\n",
      "platinum                       0\n",
      "silver                         0\n",
      "                              ..\n",
      "us_treasury_bond_etf_MA_30     0\n",
      "us_treasury_bond_etf_Std_7     0\n",
      "us_treasury_bond_etf_Std_30    0\n",
      "us_treasury_bond_etf_RoC       0\n",
      "us_treasury_bond_etf_Diff      0\n",
      "Length: 180, dtype: int64\n",
      "Values exceeding the threshold before replacement:\n",
      "Column 'interest_rates_10yr' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr, dtype: float64)\n",
      "Column 'gold' has values larger than the threshold:\n",
      "Series([], Name: gold, dtype: float64)\n",
      "Column 'copper' has values larger than the threshold:\n",
      "Series([], Name: copper, dtype: float64)\n",
      "Column 'platinum' has values larger than the threshold:\n",
      "Series([], Name: platinum, dtype: float64)\n",
      "Column 'silver' has values larger than the threshold:\n",
      "Series([], Name: silver, dtype: float64)\n",
      "Column 'crude_oil' has values larger than the threshold:\n",
      "Series([], Name: crude_oil, dtype: float64)\n",
      "Column 'natural_gas' has values larger than the threshold:\n",
      "Series([], Name: natural_gas, dtype: float64)\n",
      "Column 'corn' has values larger than the threshold:\n",
      "Series([], Name: corn, dtype: float64)\n",
      "Column 'wheat' has values larger than the threshold:\n",
      "Series([], Name: wheat, dtype: float64)\n",
      "Column 'volatility_index' has values larger than the threshold:\n",
      "Series([], Name: volatility_index, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy, dtype: float64)\n",
      "Column 'sp500' has values larger than the threshold:\n",
      "Series([], Name: sp500, dtype: float64)\n",
      "Column 'nasdaq_100' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100, dtype: float64)\n",
      "Column 'dow_jones_industrial_average' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average, dtype: float64)\n",
      "Column 'consumer_confidence_index' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf, dtype: float64)\n",
      "Column 'us_treasury_bond_etf' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf, dtype: float64)\n",
      "Column 'interest_rates_10yr_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Lag_1, dtype: float64)\n",
      "Column 'interest_rates_10yr_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Lag_2, dtype: float64)\n",
      "Column 'interest_rates_10yr_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Lag_3, dtype: float64)\n",
      "Column 'interest_rates_10yr_MA_7' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_MA_7, dtype: float64)\n",
      "Column 'interest_rates_10yr_MA_30' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_MA_30, dtype: float64)\n",
      "Column 'interest_rates_10yr_Std_7' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Std_7, dtype: float64)\n",
      "Column 'interest_rates_10yr_Std_30' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Std_30, dtype: float64)\n",
      "Column 'interest_rates_10yr_RoC' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_RoC, dtype: float64)\n",
      "Column 'interest_rates_10yr_Diff' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Diff, dtype: float64)\n",
      "Column 'gold_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: gold_Lag_1, dtype: float64)\n",
      "Column 'gold_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: gold_Lag_2, dtype: float64)\n",
      "Column 'gold_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: gold_Lag_3, dtype: float64)\n",
      "Column 'gold_MA_7' has values larger than the threshold:\n",
      "Series([], Name: gold_MA_7, dtype: float64)\n",
      "Column 'gold_MA_30' has values larger than the threshold:\n",
      "Series([], Name: gold_MA_30, dtype: float64)\n",
      "Column 'gold_Std_7' has values larger than the threshold:\n",
      "Series([], Name: gold_Std_7, dtype: float64)\n",
      "Column 'gold_Std_30' has values larger than the threshold:\n",
      "Series([], Name: gold_Std_30, dtype: float64)\n",
      "Column 'gold_RoC' has values larger than the threshold:\n",
      "Series([], Name: gold_RoC, dtype: float64)\n",
      "Column 'gold_Diff' has values larger than the threshold:\n",
      "Series([], Name: gold_Diff, dtype: float64)\n",
      "Column 'copper_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: copper_Lag_1, dtype: float64)\n",
      "Column 'copper_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: copper_Lag_2, dtype: float64)\n",
      "Column 'copper_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: copper_Lag_3, dtype: float64)\n",
      "Column 'copper_MA_7' has values larger than the threshold:\n",
      "Series([], Name: copper_MA_7, dtype: float64)\n",
      "Column 'copper_MA_30' has values larger than the threshold:\n",
      "Series([], Name: copper_MA_30, dtype: float64)\n",
      "Column 'copper_Std_7' has values larger than the threshold:\n",
      "Series([], Name: copper_Std_7, dtype: float64)\n",
      "Column 'copper_Std_30' has values larger than the threshold:\n",
      "Series([], Name: copper_Std_30, dtype: float64)\n",
      "Column 'copper_RoC' has values larger than the threshold:\n",
      "Series([], Name: copper_RoC, dtype: float64)\n",
      "Column 'copper_Diff' has values larger than the threshold:\n",
      "Series([], Name: copper_Diff, dtype: float64)\n",
      "Column 'platinum_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: platinum_Lag_1, dtype: float64)\n",
      "Column 'platinum_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: platinum_Lag_2, dtype: float64)\n",
      "Column 'platinum_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: platinum_Lag_3, dtype: float64)\n",
      "Column 'platinum_MA_7' has values larger than the threshold:\n",
      "Series([], Name: platinum_MA_7, dtype: float64)\n",
      "Column 'platinum_MA_30' has values larger than the threshold:\n",
      "Series([], Name: platinum_MA_30, dtype: float64)\n",
      "Column 'platinum_Std_7' has values larger than the threshold:\n",
      "Series([], Name: platinum_Std_7, dtype: float64)\n",
      "Column 'platinum_Std_30' has values larger than the threshold:\n",
      "Series([], Name: platinum_Std_30, dtype: float64)\n",
      "Column 'platinum_RoC' has values larger than the threshold:\n",
      "Series([], Name: platinum_RoC, dtype: float64)\n",
      "Column 'platinum_Diff' has values larger than the threshold:\n",
      "Series([], Name: platinum_Diff, dtype: float64)\n",
      "Column 'silver_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: silver_Lag_1, dtype: float64)\n",
      "Column 'silver_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: silver_Lag_2, dtype: float64)\n",
      "Column 'silver_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: silver_Lag_3, dtype: float64)\n",
      "Column 'silver_MA_7' has values larger than the threshold:\n",
      "Series([], Name: silver_MA_7, dtype: float64)\n",
      "Column 'silver_MA_30' has values larger than the threshold:\n",
      "Series([], Name: silver_MA_30, dtype: float64)\n",
      "Column 'silver_Std_7' has values larger than the threshold:\n",
      "Series([], Name: silver_Std_7, dtype: float64)\n",
      "Column 'silver_Std_30' has values larger than the threshold:\n",
      "Series([], Name: silver_Std_30, dtype: float64)\n",
      "Column 'silver_RoC' has values larger than the threshold:\n",
      "Series([], Name: silver_RoC, dtype: float64)\n",
      "Column 'silver_Diff' has values larger than the threshold:\n",
      "Series([], Name: silver_Diff, dtype: float64)\n",
      "Column 'crude_oil_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Lag_1, dtype: float64)\n",
      "Column 'crude_oil_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Lag_2, dtype: float64)\n",
      "Column 'crude_oil_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Lag_3, dtype: float64)\n",
      "Column 'crude_oil_MA_7' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_MA_7, dtype: float64)\n",
      "Column 'crude_oil_MA_30' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_MA_30, dtype: float64)\n",
      "Column 'crude_oil_Std_7' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Std_7, dtype: float64)\n",
      "Column 'crude_oil_Std_30' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Std_30, dtype: float64)\n",
      "Column 'crude_oil_RoC' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_RoC, dtype: float64)\n",
      "Column 'crude_oil_Diff' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Diff, dtype: float64)\n",
      "Column 'natural_gas_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Lag_1, dtype: float64)\n",
      "Column 'natural_gas_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Lag_2, dtype: float64)\n",
      "Column 'natural_gas_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Lag_3, dtype: float64)\n",
      "Column 'natural_gas_MA_7' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_MA_7, dtype: float64)\n",
      "Column 'natural_gas_MA_30' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_MA_30, dtype: float64)\n",
      "Column 'natural_gas_Std_7' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Std_7, dtype: float64)\n",
      "Column 'natural_gas_Std_30' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Std_30, dtype: float64)\n",
      "Column 'natural_gas_RoC' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_RoC, dtype: float64)\n",
      "Column 'natural_gas_Diff' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Diff, dtype: float64)\n",
      "Column 'corn_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: corn_Lag_1, dtype: float64)\n",
      "Column 'corn_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: corn_Lag_2, dtype: float64)\n",
      "Column 'corn_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: corn_Lag_3, dtype: float64)\n",
      "Column 'corn_MA_7' has values larger than the threshold:\n",
      "Series([], Name: corn_MA_7, dtype: float64)\n",
      "Column 'corn_MA_30' has values larger than the threshold:\n",
      "Series([], Name: corn_MA_30, dtype: float64)\n",
      "Column 'corn_Std_7' has values larger than the threshold:\n",
      "Series([], Name: corn_Std_7, dtype: float64)\n",
      "Column 'corn_Std_30' has values larger than the threshold:\n",
      "Series([], Name: corn_Std_30, dtype: float64)\n",
      "Column 'corn_RoC' has values larger than the threshold:\n",
      "Series([], Name: corn_RoC, dtype: float64)\n",
      "Column 'corn_Diff' has values larger than the threshold:\n",
      "Series([], Name: corn_Diff, dtype: float64)\n",
      "Column 'wheat_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: wheat_Lag_1, dtype: float64)\n",
      "Column 'wheat_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: wheat_Lag_2, dtype: float64)\n",
      "Column 'wheat_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: wheat_Lag_3, dtype: float64)\n",
      "Column 'wheat_MA_7' has values larger than the threshold:\n",
      "Series([], Name: wheat_MA_7, dtype: float64)\n",
      "Column 'wheat_MA_30' has values larger than the threshold:\n",
      "Series([], Name: wheat_MA_30, dtype: float64)\n",
      "Column 'wheat_Std_7' has values larger than the threshold:\n",
      "Series([], Name: wheat_Std_7, dtype: float64)\n",
      "Column 'wheat_Std_30' has values larger than the threshold:\n",
      "Series([], Name: wheat_Std_30, dtype: float64)\n",
      "Column 'wheat_RoC' has values larger than the threshold:\n",
      "Series([], Name: wheat_RoC, dtype: float64)\n",
      "Column 'wheat_Diff' has values larger than the threshold:\n",
      "Series([], Name: wheat_Diff, dtype: float64)\n",
      "Column 'volatility_index_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Lag_1, dtype: float64)\n",
      "Column 'volatility_index_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Lag_2, dtype: float64)\n",
      "Column 'volatility_index_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Lag_3, dtype: float64)\n",
      "Column 'volatility_index_MA_7' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_MA_7, dtype: float64)\n",
      "Column 'volatility_index_MA_30' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_MA_30, dtype: float64)\n",
      "Column 'volatility_index_Std_7' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Std_7, dtype: float64)\n",
      "Column 'volatility_index_Std_30' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Std_30, dtype: float64)\n",
      "Column 'volatility_index_RoC' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_RoC, dtype: float64)\n",
      "Column 'volatility_index_Diff' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Diff, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Lag_1, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Lag_2, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Lag_3, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_MA_7' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_MA_7, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_MA_30' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_MA_30, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Std_7' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Std_7, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Std_30' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Std_30, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_RoC' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_RoC, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Diff' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Diff, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Lag_1, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Lag_2, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Lag_3, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_MA_7' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_MA_7, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_MA_30' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_MA_30, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Std_7' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Std_7, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Std_30' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Std_30, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_RoC' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_RoC, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Diff' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Diff, dtype: float64)\n",
      "Column 'sp500_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: sp500_Lag_1, dtype: float64)\n",
      "Column 'sp500_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: sp500_Lag_2, dtype: float64)\n",
      "Column 'sp500_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: sp500_Lag_3, dtype: float64)\n",
      "Column 'sp500_MA_7' has values larger than the threshold:\n",
      "Series([], Name: sp500_MA_7, dtype: float64)\n",
      "Column 'sp500_MA_30' has values larger than the threshold:\n",
      "Series([], Name: sp500_MA_30, dtype: float64)\n",
      "Column 'sp500_Std_7' has values larger than the threshold:\n",
      "Series([], Name: sp500_Std_7, dtype: float64)\n",
      "Column 'sp500_Std_30' has values larger than the threshold:\n",
      "Series([], Name: sp500_Std_30, dtype: float64)\n",
      "Column 'sp500_RoC' has values larger than the threshold:\n",
      "Series([], Name: sp500_RoC, dtype: float64)\n",
      "Column 'sp500_Diff' has values larger than the threshold:\n",
      "Series([], Name: sp500_Diff, dtype: float64)\n",
      "Column 'nasdaq_100_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Lag_1, dtype: float64)\n",
      "Column 'nasdaq_100_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Lag_2, dtype: float64)\n",
      "Column 'nasdaq_100_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Lag_3, dtype: float64)\n",
      "Column 'nasdaq_100_MA_7' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_MA_7, dtype: float64)\n",
      "Column 'nasdaq_100_MA_30' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_MA_30, dtype: float64)\n",
      "Column 'nasdaq_100_Std_7' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Std_7, dtype: float64)\n",
      "Column 'nasdaq_100_Std_30' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Std_30, dtype: float64)\n",
      "Column 'nasdaq_100_RoC' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_RoC, dtype: float64)\n",
      "Column 'nasdaq_100_Diff' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Diff, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Lag_1, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Lag_2, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Lag_3, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_MA_7' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_MA_7, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_MA_30' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_MA_30, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Std_7' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Std_7, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Std_30' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Std_30, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_RoC' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_RoC, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Diff' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Diff, dtype: float64)\n",
      "Column 'consumer_confidence_index_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Lag_1, dtype: float64)\n",
      "Column 'consumer_confidence_index_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Lag_2, dtype: float64)\n",
      "Column 'consumer_confidence_index_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Lag_3, dtype: float64)\n",
      "Column 'consumer_confidence_index_MA_7' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_MA_7, dtype: float64)\n",
      "Column 'consumer_confidence_index_MA_30' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_MA_30, dtype: float64)\n",
      "Column 'consumer_confidence_index_Std_7' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Std_7, dtype: float64)\n",
      "Column 'consumer_confidence_index_Std_30' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Std_30, dtype: float64)\n",
      "Column 'consumer_confidence_index_RoC' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_RoC, dtype: float64)\n",
      "Column 'consumer_confidence_index_Diff' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Diff, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Lag_1, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Lag_2, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Lag_3, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_MA_7' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_MA_7, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_MA_30' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_MA_30, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Std_7' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Std_7, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Std_30' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Std_30, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_RoC' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_RoC, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Diff' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Diff, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Lag_1, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Lag_2, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Lag_3, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_MA_7' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_MA_7, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_MA_30' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_MA_30, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Std_7' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Std_7, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Std_30' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Std_30, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_RoC' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_RoC, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Diff' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Diff, dtype: float64)\n",
      "Max value after replacement 41198.078125\n",
      "Min value after replacement: 1e-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fill_missing_vals(df):\n",
    "    # Ensure we're only applying the logic to numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Replace any 0s or negative values with extremely small positive integers\n",
    "    numeric_cols = numeric_cols.apply(lambda x: np.where(x <= 0, 1e-10, x))\n",
    "    \n",
    "    # Impute missing values with the following methodologies    \n",
    "    numeric_cols.ffill(inplace=True)\n",
    "    numeric_cols.bfill(inplace=True)\n",
    "    numeric_cols.interpolate(method='linear', inplace=True)\n",
    "    \n",
    "    # Update the original dataframe with the modified numeric columns\n",
    "    df[numeric_cols.columns] = numeric_cols\n",
    "    \n",
    "    return df\n",
    "\n",
    "exo_data = exo_data.copy()\n",
    "\n",
    "# Apply fill_missing_vals only to numeric columns with missing values\n",
    "exo_data = fill_missing_vals(exo_data)\n",
    "\n",
    "print(\"Missing values handled:\")\n",
    "print(exo_data.isna().sum())\n",
    "\n",
    "# We will also put in a check for extremely large values as we are seeing warnings for those below\n",
    "def replace_large_values(df, threshold=1e9):\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Apply logic only to numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Debug print statement to see what and how many values exceed our given threshold.\n",
    "    print(\"Values exceeding the threshold before replacement:\")\n",
    "    for col in numeric_cols.columns:\n",
    "        large_vals = df[col] > threshold\n",
    "        if not large_vals.any():\n",
    "            print(f\"Column '{col}' has values larger than the threshold:\")\n",
    "            print(df[col][large_vals])\n",
    "    \n",
    "    # Replace large values with NaN\n",
    "    df[numeric_cols.columns] = numeric_cols.apply(lambda x: np.where(x > threshold, np.nan, x))\n",
    "    \n",
    "    # Apply the fill_missing_vals logic to impute the modified numeric columns\n",
    "    df = fill_missing_vals(numeric_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "threshold_value = 1e9\n",
    "exo_stock_data = replace_large_values(exo_data, threshold=threshold_value)\n",
    "\n",
    "# Calculate the max and min values only for numeric columns\n",
    "numeric_cols = exo_data.select_dtypes(include = [np.number])\n",
    "\n",
    "print(\"Max value after replacement\", numeric_cols.max().max())\n",
    "print(\"Min value after replacement:\", numeric_cols.min().min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great, now let's look into viewing and handling outliers.  We will first see the appearance of outliers using z_score with a standard std of 3, and then looking at the column feature spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_rates_10yr             0\n",
      "gold                            0\n",
      "copper                          0\n",
      "platinum                        8\n",
      "silver                          0\n",
      "                               ..\n",
      "us_treasury_bond_etf_MA_30      0\n",
      "us_treasury_bond_etf_Std_7     23\n",
      "us_treasury_bond_etf_Std_30    20\n",
      "us_treasury_bond_etf_RoC       32\n",
      "us_treasury_bond_etf_Diff      28\n",
      "Length: 180, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numeric_cols = exo_data.select_dtypes(include = [np.number])\n",
    "z_scores = np.abs(stats.zscore(numeric_cols))\n",
    "threshold = 3 # Common threshold starting std modifier\n",
    "\n",
    "outliers = (z_scores > threshold)\n",
    "\n",
    "print(outliers.sum(axis = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is great, just using a standard zscore we are able to isolate the vast majority of potential outliers.  No further activity is needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the last phase of our preprocessing notebook we can now move on to scaling our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First though we will create a save point and a separate csv of our unscaled secondary stock data, as we will need this in an upcoming notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_data.to_csv(os.path.join(project_root, 'data', 'exo_data_unscaled.csv'), index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            interest_rates_10yr      gold    copper  platinum    silver  \\\n",
      "Date                                                                      \n",
      "2019-01-01             0.129662 -2.016464 -1.367923 -1.474316 -1.547060   \n",
      "2019-01-02             0.129662 -2.016464 -1.367923 -1.474316 -1.547060   \n",
      "2019-01-03             0.045900 -1.974246 -1.444227 -1.517995 -1.507817   \n",
      "2019-01-04             0.128096 -2.009818 -1.330821 -1.256867 -1.510450   \n",
      "2019-01-07             0.146101 -1.993791 -1.345522 -1.291051 -1.516671   \n",
      "\n",
      "            crude_oil  natural_gas      corn     wheat  volatility_index  ...  \\\n",
      "Date                                                                      ...   \n",
      "2019-01-01  -1.119621    -0.231492 -0.983181 -0.917486          0.330896  ...   \n",
      "2019-01-02  -1.119621    -0.231492 -0.983181 -0.917486          0.330896  ...   \n",
      "2019-01-03  -1.091659    -0.238798 -0.953352 -0.872560          0.607129  ...   \n",
      "2019-01-04  -1.047429    -0.183159 -0.929115 -0.851702          0.102973  ...   \n",
      "2019-01-07  -1.018959    -0.239360 -0.934708 -0.853306          0.105450  ...   \n",
      "\n",
      "            vanguard_total_world_stock_etf_Diff  us_treasury_bond_etf_Lag_1  \\\n",
      "Date                                                                          \n",
      "2019-01-01                            -0.633745                   -0.089940   \n",
      "2019-01-02                            -0.633745                   -0.089940   \n",
      "2019-01-03                            -0.633745                   -0.089940   \n",
      "2019-01-04                             3.068413                   -0.018155   \n",
      "2019-01-07                             0.092524                   -0.084813   \n",
      "\n",
      "            us_treasury_bond_etf_Lag_2  us_treasury_bond_etf_Lag_3  \\\n",
      "Date                                                                 \n",
      "2019-01-01                   -0.090517                   -0.091125   \n",
      "2019-01-02                   -0.090517                   -0.091125   \n",
      "2019-01-03                   -0.090517                   -0.091125   \n",
      "2019-01-04                   -0.090517                   -0.091125   \n",
      "2019-01-07                   -0.018712                   -0.091125   \n",
      "\n",
      "            us_treasury_bond_etf_MA_7  us_treasury_bond_etf_MA_30  \\\n",
      "Date                                                                \n",
      "2019-01-01                  -0.096364                   -0.128119   \n",
      "2019-01-02                  -0.096364                   -0.128119   \n",
      "2019-01-03                  -0.096364                   -0.128119   \n",
      "2019-01-04                  -0.096364                   -0.128119   \n",
      "2019-01-07                  -0.096364                   -0.128119   \n",
      "\n",
      "            us_treasury_bond_etf_Std_7  us_treasury_bond_etf_Std_30  \\\n",
      "Date                                                                  \n",
      "2019-01-01                   -0.170434                    -1.196171   \n",
      "2019-01-02                   -0.170434                    -1.196171   \n",
      "2019-01-03                   -0.170434                    -1.196171   \n",
      "2019-01-04                   -0.170434                    -1.196171   \n",
      "2019-01-07                   -0.170434                    -1.196171   \n",
      "\n",
      "            us_treasury_bond_etf_RoC  us_treasury_bond_etf_Diff  \n",
      "Date                                                             \n",
      "2019-01-01                 -0.586537                  -0.585339  \n",
      "2019-01-02                 -0.586537                  -0.585339  \n",
      "2019-01-03                  1.887533                   1.898984  \n",
      "2019-01-04                 -0.586537                  -0.585339  \n",
      "2019-01-07                 -0.586537                  -0.585339  \n",
      "\n",
      "[5 rows x 180 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initiate the scaler and transform the data.\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(exo_data)\n",
    "exo_scaled = pd.DataFrame(scaled_data, index = exo_data.index, columns = exo_data.columns)\n",
    "\n",
    "print(exo_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This looks good, let's save it so we can use later on in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "exo_scaled.to_csv(os.path.join(project_root, 'data', 'exo_data_preprocessed.csv'), index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary we have taken our raw secondary stock data that we generated from our generate_secondary_stocks.py script and have preprocessed it for further purposes.  We have created a 3-day lag window and our custom features, addressed any missing values and imputed accordingly, and looked at outliers a bit and making sure that we established normal distribution in at least a few of our features for good measure.  We ended this notebook by scaling our new dataframe using StandardScaler so that it is ready to go upon next use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Strat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
