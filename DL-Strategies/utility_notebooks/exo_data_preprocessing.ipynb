{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exogenous Data Preprocessing Notebook**\n",
    "## In this notebook we will preprocess our exogenous data, first creating our lag windows and the custom features that come from expanding the resulting view.  Then we will do any necessary cleaning such as reviewing outliers and missing values in our expanded data, imputing as needed so we have a clean dataset going into the next phase of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by bringing in the libraries and logic necessary for reading in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's read in our data that we need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            interest_rates_10yr         gold  copper    platinum  silver  \\\n",
      "Date                                                                       \n",
      "2019-03-14                2.630  1293.400024  2.8885  825.599976  15.101   \n",
      "2019-03-15                2.593  1301.800049  2.9035  830.299988  15.253   \n",
      "2019-03-18                2.602  1300.300049  2.9035  832.599976  15.251   \n",
      "2019-03-19                2.614  1305.000000  2.9195  851.200012  15.301   \n",
      "2019-03-20                2.535  1300.500000  2.9155  858.200012  15.245   \n",
      "\n",
      "            crude_oil  natural_gas    corn   wheat  volatility_index  \\\n",
      "Date                                                                   \n",
      "2019-03-14  58.610001        2.855  361.50  448.25             13.50   \n",
      "2019-03-15  58.520000        2.795  373.25  462.25             12.88   \n",
      "2019-03-18  59.090000        2.850  371.50  456.75             13.10   \n",
      "2019-03-19  59.029999        2.874  371.25  456.50             13.56   \n",
      "2019-03-20  59.830002        2.820  371.50  464.75             13.91   \n",
      "\n",
      "            exchange_rate_usd_eur  exchange_rate_usd_jpy        sp500  \\\n",
      "Date                                                                    \n",
      "2019-03-14               1.133106             111.195999  2808.479980   \n",
      "2019-03-15               1.130838             111.752998  2822.479980   \n",
      "2019-03-18               1.131977             111.560997  2832.939941   \n",
      "2019-03-19               1.133620             111.373001  2832.570068   \n",
      "2019-03-20               1.135486             111.401001  2824.229980   \n",
      "\n",
      "             nasdaq_100  dow_jones_industrial_average  \\\n",
      "Date                                                    \n",
      "2019-03-14  7243.009766                  25709.939453   \n",
      "2019-03-15  7306.990234                  25848.869141   \n",
      "2019-03-18  7326.279785                  25914.099609   \n",
      "2019-03-19  7349.279785                  25887.380859   \n",
      "2019-03-20  7380.750000                  25745.669922   \n",
      "\n",
      "            consumer_confidence_index  vanguard_total_world_stock_etf  \\\n",
      "Date                                                                    \n",
      "2019-03-14                 124.010002                       73.000000   \n",
      "2019-03-15                 125.029999                       73.570000   \n",
      "2019-03-18                 124.660004                       73.900002   \n",
      "2019-03-19                 124.570000                       73.910004   \n",
      "2019-03-20                 125.760002                       73.790001   \n",
      "\n",
      "            us_treasury_bond_etf  \n",
      "Date                              \n",
      "2019-03-14             24.870001  \n",
      "2019-03-15             24.920000  \n",
      "2019-03-18             24.910000  \n",
      "2019-03-19             24.900000  \n",
      "2019-03-20             25.010000  \n",
      "(1415, 18)\n"
     ]
    }
   ],
   "source": [
    "# Now let's access the main core_stock_data.csv file\n",
    "csv_path = os.path.join(project_root, 'data', 'exogenous_data.csv')\n",
    "exo_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "print(exo_data.head())\n",
    "print(exo_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step I want to undertake is handling and imputing any missing features.  I want to do this first so that when we create our lag windows and resulting custom features the new values will be calculated properly, and it will be easier to address our resulting outliers.  We will look at the same process that we used for our core stock data, a combination of ffill, bfill and linear interpolation for max efficiency of our missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_rates_10yr               0\n",
      "gold                              0\n",
      "copper                            0\n",
      "platinum                          0\n",
      "silver                            0\n",
      "crude_oil                         0\n",
      "natural_gas                       0\n",
      "corn                              0\n",
      "wheat                             0\n",
      "volatility_index                  0\n",
      "exchange_rate_usd_eur             0\n",
      "exchange_rate_usd_jpy             0\n",
      "sp500                             0\n",
      "nasdaq_100                        0\n",
      "dow_jones_industrial_average      0\n",
      "consumer_confidence_index         0\n",
      "vanguard_total_world_stock_etf    0\n",
      "us_treasury_bond_etf              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def fill_missing_vals(df):\n",
    "    df.ffill(inplace = True)\n",
    "    df.bfill(inplace = True)\n",
    "    df.interpolate(method = 'linear', inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "exo_data = exo_data.copy()\n",
    "\n",
    "for col in exo_data.columns:\n",
    "    if exo_data[col].isnull().any():\n",
    "        exo_data.loc[:, col] = fill_missing_vals(exo_data[[col]])[col]\n",
    "\n",
    "\n",
    "\n",
    "print(exo_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks great, now let's create the lag windows and custom features for our secondary stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1415, 180)\n",
      "            interest_rates_10yr         gold  copper    platinum  silver  \\\n",
      "Date                                                                       \n",
      "2019-03-14                2.630  1293.400024  2.8885  825.599976  15.101   \n",
      "2019-03-15                2.593  1301.800049  2.9035  830.299988  15.253   \n",
      "2019-03-18                2.602  1300.300049  2.9035  832.599976  15.251   \n",
      "2019-03-19                2.614  1305.000000  2.9195  851.200012  15.301   \n",
      "2019-03-20                2.535  1300.500000  2.9155  858.200012  15.245   \n",
      "\n",
      "            crude_oil  natural_gas    corn   wheat  volatility_index  ...  \\\n",
      "Date                                                                  ...   \n",
      "2019-03-14  58.610001        2.855  361.50  448.25             13.50  ...   \n",
      "2019-03-15  58.520000        2.795  373.25  462.25             12.88  ...   \n",
      "2019-03-18  59.090000        2.850  371.50  456.75             13.10  ...   \n",
      "2019-03-19  59.029999        2.874  371.25  456.50             13.56  ...   \n",
      "2019-03-20  59.830002        2.820  371.50  464.75             13.91  ...   \n",
      "\n",
      "            vanguard_total_world_stock_etf_Diff  us_treasury_bond_etf_Lag_1  \\\n",
      "Date                                                                          \n",
      "2019-03-14                                  NaN                         NaN   \n",
      "2019-03-15                             0.570000                   24.870001   \n",
      "2019-03-18                             0.330002                   24.920000   \n",
      "2019-03-19                             0.010002                   24.910000   \n",
      "2019-03-20                            -0.120003                   24.900000   \n",
      "\n",
      "            us_treasury_bond_etf_Lag_2  us_treasury_bond_etf_Lag_3  \\\n",
      "Date                                                                 \n",
      "2019-03-14                         NaN                         NaN   \n",
      "2019-03-15                         NaN                         NaN   \n",
      "2019-03-18                   24.870001                         NaN   \n",
      "2019-03-19                   24.920000                   24.870001   \n",
      "2019-03-20                   24.910000                   24.920000   \n",
      "\n",
      "            us_treasury_bond_etf_MA_7  us_treasury_bond_etf_MA_30  \\\n",
      "Date                                                                \n",
      "2019-03-14                        NaN                         NaN   \n",
      "2019-03-15                        NaN                         NaN   \n",
      "2019-03-18                        NaN                         NaN   \n",
      "2019-03-19                        NaN                         NaN   \n",
      "2019-03-20                        NaN                         NaN   \n",
      "\n",
      "            us_treasury_bond_etf_Std_7  us_treasury_bond_etf_Std_30  \\\n",
      "Date                                                                  \n",
      "2019-03-14                         NaN                          NaN   \n",
      "2019-03-15                         NaN                          NaN   \n",
      "2019-03-18                         NaN                          NaN   \n",
      "2019-03-19                         NaN                          NaN   \n",
      "2019-03-20                         NaN                          NaN   \n",
      "\n",
      "            us_treasury_bond_etf_RoC  us_treasury_bond_etf_Diff  \n",
      "Date                                                             \n",
      "2019-03-14                       NaN                        NaN  \n",
      "2019-03-15                  0.201042                   0.049999  \n",
      "2019-03-18                 -0.040129                  -0.010000  \n",
      "2019-03-19                 -0.040145                  -0.010000  \n",
      "2019-03-20                  0.441770                   0.110001  \n",
      "\n",
      "[5 rows x 180 columns]\n",
      "interest_rates_10yr             0\n",
      "gold                            0\n",
      "copper                          0\n",
      "platinum                        0\n",
      "silver                          0\n",
      "                               ..\n",
      "us_treasury_bond_etf_MA_30     29\n",
      "us_treasury_bond_etf_Std_7      6\n",
      "us_treasury_bond_etf_Std_30    29\n",
      "us_treasury_bond_etf_RoC        1\n",
      "us_treasury_bond_etf_Diff       1\n",
      "Length: 180, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_1748\\3770416175.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n"
     ]
    }
   ],
   "source": [
    "# We will need to create a temp variable since we are making so many custom features, it will otherwise create a PerformanceWarning since when we execute the cell on our main df its too much at once.\n",
    "# By storing all the custom features to a temp variable and then concatenating them in a separate step it takes the pressure off in the execute phase.\n",
    "new_temp_columns = pd.DataFrame(index = exo_data.index)\n",
    "\n",
    "# Now let's create a variable to house our features we want to create lag features for.\n",
    "features_to_lag = ['interest_rates_10yr', 'gold', 'copper', 'platinum', 'silver', 'crude_oil',\n",
    "                    'natural_gas', 'corn', 'wheat', 'volatility_index', 'exchange_rate_usd_eur',\n",
    "                    'exchange_rate_usd_jpy', 'sp500', 'nasdaq_100', 'dow_jones_industrial_average',\n",
    "                    'consumer_confidence_index', 'vanguard_total_world_stock_etf', 'us_treasury_bond_etf']\n",
    "\n",
    "# Now we apply the lag windows with a small 3-day window size.\n",
    "for feature in features_to_lag:\n",
    "    for lag in range(1,4):\n",
    "        new_temp_columns[f'{feature}_Lag_{lag}'] = exo_data[feature].shift(lag)\n",
    "\n",
    "# Great, now from these new lag-based features let's expand on it further by creating some custom features using our existing ones.\n",
    "# Moving Averages\n",
    "    new_temp_columns[f'{feature}_MA_7'] = exo_data[feature].rolling(window=7).mean()\n",
    "    new_temp_columns[f'{feature}_MA_30'] = exo_data[feature].rolling(window=30).mean()\n",
    "    # Volatility\n",
    "    new_temp_columns[f'{feature}_Std_7'] = exo_data[feature].rolling(window=7).std()\n",
    "    new_temp_columns[f'{feature}_Std_30'] = exo_data[feature].rolling(window=30).std()\n",
    "    # Rate of Change (Momentum)\n",
    "    new_temp_columns[f'{feature}_RoC'] = exo_data[feature].pct_change() * 100\n",
    "    # Difference Features\n",
    "    new_temp_columns[f'{feature}_Diff'] = exo_data[feature].diff()\n",
    "\n",
    "# Now we can concatenate onto our original core_stock_data.\n",
    "exo_data = pd.concat([exo_data, new_temp_columns], axis = 1)\n",
    "\n",
    "print(exo_data.shape)\n",
    "print(exo_data.head())\n",
    "\n",
    "print(exo_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It looks like through the creation of all of our new features there are just a few missing values that slipped through.  Let's re-run our imputation logic to take care of them.  Also since I see 0s in our .head() printout, we will address those in our function below as leaving those will cause issues down the road with our outlier calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled:\n",
      "interest_rates_10yr            0\n",
      "gold                           0\n",
      "copper                         0\n",
      "platinum                       0\n",
      "silver                         0\n",
      "                              ..\n",
      "us_treasury_bond_etf_MA_30     0\n",
      "us_treasury_bond_etf_Std_7     0\n",
      "us_treasury_bond_etf_Std_30    0\n",
      "us_treasury_bond_etf_RoC       0\n",
      "us_treasury_bond_etf_Diff      0\n",
      "Length: 180, dtype: int64\n",
      "Values exceeding the threshold before replacement:\n",
      "Column 'interest_rates_10yr' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr, dtype: float64)\n",
      "Column 'gold' has values larger than the threshold:\n",
      "Series([], Name: gold, dtype: float64)\n",
      "Column 'copper' has values larger than the threshold:\n",
      "Series([], Name: copper, dtype: float64)\n",
      "Column 'platinum' has values larger than the threshold:\n",
      "Series([], Name: platinum, dtype: float64)\n",
      "Column 'silver' has values larger than the threshold:\n",
      "Series([], Name: silver, dtype: float64)\n",
      "Column 'crude_oil' has values larger than the threshold:\n",
      "Series([], Name: crude_oil, dtype: float64)\n",
      "Column 'natural_gas' has values larger than the threshold:\n",
      "Series([], Name: natural_gas, dtype: float64)\n",
      "Column 'corn' has values larger than the threshold:\n",
      "Series([], Name: corn, dtype: float64)\n",
      "Column 'wheat' has values larger than the threshold:\n",
      "Series([], Name: wheat, dtype: float64)\n",
      "Column 'volatility_index' has values larger than the threshold:\n",
      "Series([], Name: volatility_index, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy, dtype: float64)\n",
      "Column 'sp500' has values larger than the threshold:\n",
      "Series([], Name: sp500, dtype: float64)\n",
      "Column 'nasdaq_100' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100, dtype: float64)\n",
      "Column 'dow_jones_industrial_average' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average, dtype: float64)\n",
      "Column 'consumer_confidence_index' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf, dtype: float64)\n",
      "Column 'us_treasury_bond_etf' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf, dtype: float64)\n",
      "Column 'interest_rates_10yr_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Lag_1, dtype: float64)\n",
      "Column 'interest_rates_10yr_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Lag_2, dtype: float64)\n",
      "Column 'interest_rates_10yr_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Lag_3, dtype: float64)\n",
      "Column 'interest_rates_10yr_MA_7' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_MA_7, dtype: float64)\n",
      "Column 'interest_rates_10yr_MA_30' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_MA_30, dtype: float64)\n",
      "Column 'interest_rates_10yr_Std_7' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Std_7, dtype: float64)\n",
      "Column 'interest_rates_10yr_Std_30' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Std_30, dtype: float64)\n",
      "Column 'interest_rates_10yr_RoC' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_RoC, dtype: float64)\n",
      "Column 'interest_rates_10yr_Diff' has values larger than the threshold:\n",
      "Series([], Name: interest_rates_10yr_Diff, dtype: float64)\n",
      "Column 'gold_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: gold_Lag_1, dtype: float64)\n",
      "Column 'gold_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: gold_Lag_2, dtype: float64)\n",
      "Column 'gold_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: gold_Lag_3, dtype: float64)\n",
      "Column 'gold_MA_7' has values larger than the threshold:\n",
      "Series([], Name: gold_MA_7, dtype: float64)\n",
      "Column 'gold_MA_30' has values larger than the threshold:\n",
      "Series([], Name: gold_MA_30, dtype: float64)\n",
      "Column 'gold_Std_7' has values larger than the threshold:\n",
      "Series([], Name: gold_Std_7, dtype: float64)\n",
      "Column 'gold_Std_30' has values larger than the threshold:\n",
      "Series([], Name: gold_Std_30, dtype: float64)\n",
      "Column 'gold_RoC' has values larger than the threshold:\n",
      "Series([], Name: gold_RoC, dtype: float64)\n",
      "Column 'gold_Diff' has values larger than the threshold:\n",
      "Series([], Name: gold_Diff, dtype: float64)\n",
      "Column 'copper_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: copper_Lag_1, dtype: float64)\n",
      "Column 'copper_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: copper_Lag_2, dtype: float64)\n",
      "Column 'copper_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: copper_Lag_3, dtype: float64)\n",
      "Column 'copper_MA_7' has values larger than the threshold:\n",
      "Series([], Name: copper_MA_7, dtype: float64)\n",
      "Column 'copper_MA_30' has values larger than the threshold:\n",
      "Series([], Name: copper_MA_30, dtype: float64)\n",
      "Column 'copper_Std_7' has values larger than the threshold:\n",
      "Series([], Name: copper_Std_7, dtype: float64)\n",
      "Column 'copper_Std_30' has values larger than the threshold:\n",
      "Series([], Name: copper_Std_30, dtype: float64)\n",
      "Column 'copper_RoC' has values larger than the threshold:\n",
      "Series([], Name: copper_RoC, dtype: float64)\n",
      "Column 'copper_Diff' has values larger than the threshold:\n",
      "Series([], Name: copper_Diff, dtype: float64)\n",
      "Column 'platinum_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: platinum_Lag_1, dtype: float64)\n",
      "Column 'platinum_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: platinum_Lag_2, dtype: float64)\n",
      "Column 'platinum_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: platinum_Lag_3, dtype: float64)\n",
      "Column 'platinum_MA_7' has values larger than the threshold:\n",
      "Series([], Name: platinum_MA_7, dtype: float64)\n",
      "Column 'platinum_MA_30' has values larger than the threshold:\n",
      "Series([], Name: platinum_MA_30, dtype: float64)\n",
      "Column 'platinum_Std_7' has values larger than the threshold:\n",
      "Series([], Name: platinum_Std_7, dtype: float64)\n",
      "Column 'platinum_Std_30' has values larger than the threshold:\n",
      "Series([], Name: platinum_Std_30, dtype: float64)\n",
      "Column 'platinum_RoC' has values larger than the threshold:\n",
      "Series([], Name: platinum_RoC, dtype: float64)\n",
      "Column 'platinum_Diff' has values larger than the threshold:\n",
      "Series([], Name: platinum_Diff, dtype: float64)\n",
      "Column 'silver_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: silver_Lag_1, dtype: float64)\n",
      "Column 'silver_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: silver_Lag_2, dtype: float64)\n",
      "Column 'silver_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: silver_Lag_3, dtype: float64)\n",
      "Column 'silver_MA_7' has values larger than the threshold:\n",
      "Series([], Name: silver_MA_7, dtype: float64)\n",
      "Column 'silver_MA_30' has values larger than the threshold:\n",
      "Series([], Name: silver_MA_30, dtype: float64)\n",
      "Column 'silver_Std_7' has values larger than the threshold:\n",
      "Series([], Name: silver_Std_7, dtype: float64)\n",
      "Column 'silver_Std_30' has values larger than the threshold:\n",
      "Series([], Name: silver_Std_30, dtype: float64)\n",
      "Column 'silver_RoC' has values larger than the threshold:\n",
      "Series([], Name: silver_RoC, dtype: float64)\n",
      "Column 'silver_Diff' has values larger than the threshold:\n",
      "Series([], Name: silver_Diff, dtype: float64)\n",
      "Column 'crude_oil_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Lag_1, dtype: float64)\n",
      "Column 'crude_oil_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Lag_2, dtype: float64)\n",
      "Column 'crude_oil_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Lag_3, dtype: float64)\n",
      "Column 'crude_oil_MA_7' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_MA_7, dtype: float64)\n",
      "Column 'crude_oil_MA_30' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_MA_30, dtype: float64)\n",
      "Column 'crude_oil_Std_7' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Std_7, dtype: float64)\n",
      "Column 'crude_oil_Std_30' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Std_30, dtype: float64)\n",
      "Column 'crude_oil_RoC' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_RoC, dtype: float64)\n",
      "Column 'crude_oil_Diff' has values larger than the threshold:\n",
      "Series([], Name: crude_oil_Diff, dtype: float64)\n",
      "Column 'natural_gas_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Lag_1, dtype: float64)\n",
      "Column 'natural_gas_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Lag_2, dtype: float64)\n",
      "Column 'natural_gas_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Lag_3, dtype: float64)\n",
      "Column 'natural_gas_MA_7' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_MA_7, dtype: float64)\n",
      "Column 'natural_gas_MA_30' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_MA_30, dtype: float64)\n",
      "Column 'natural_gas_Std_7' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Std_7, dtype: float64)\n",
      "Column 'natural_gas_Std_30' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Std_30, dtype: float64)\n",
      "Column 'natural_gas_RoC' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_RoC, dtype: float64)\n",
      "Column 'natural_gas_Diff' has values larger than the threshold:\n",
      "Series([], Name: natural_gas_Diff, dtype: float64)\n",
      "Column 'corn_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: corn_Lag_1, dtype: float64)\n",
      "Column 'corn_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: corn_Lag_2, dtype: float64)\n",
      "Column 'corn_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: corn_Lag_3, dtype: float64)\n",
      "Column 'corn_MA_7' has values larger than the threshold:\n",
      "Series([], Name: corn_MA_7, dtype: float64)\n",
      "Column 'corn_MA_30' has values larger than the threshold:\n",
      "Series([], Name: corn_MA_30, dtype: float64)\n",
      "Column 'corn_Std_7' has values larger than the threshold:\n",
      "Series([], Name: corn_Std_7, dtype: float64)\n",
      "Column 'corn_Std_30' has values larger than the threshold:\n",
      "Series([], Name: corn_Std_30, dtype: float64)\n",
      "Column 'corn_RoC' has values larger than the threshold:\n",
      "Series([], Name: corn_RoC, dtype: float64)\n",
      "Column 'corn_Diff' has values larger than the threshold:\n",
      "Series([], Name: corn_Diff, dtype: float64)\n",
      "Column 'wheat_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: wheat_Lag_1, dtype: float64)\n",
      "Column 'wheat_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: wheat_Lag_2, dtype: float64)\n",
      "Column 'wheat_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: wheat_Lag_3, dtype: float64)\n",
      "Column 'wheat_MA_7' has values larger than the threshold:\n",
      "Series([], Name: wheat_MA_7, dtype: float64)\n",
      "Column 'wheat_MA_30' has values larger than the threshold:\n",
      "Series([], Name: wheat_MA_30, dtype: float64)\n",
      "Column 'wheat_Std_7' has values larger than the threshold:\n",
      "Series([], Name: wheat_Std_7, dtype: float64)\n",
      "Column 'wheat_Std_30' has values larger than the threshold:\n",
      "Series([], Name: wheat_Std_30, dtype: float64)\n",
      "Column 'wheat_RoC' has values larger than the threshold:\n",
      "Series([], Name: wheat_RoC, dtype: float64)\n",
      "Column 'wheat_Diff' has values larger than the threshold:\n",
      "Series([], Name: wheat_Diff, dtype: float64)\n",
      "Column 'volatility_index_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Lag_1, dtype: float64)\n",
      "Column 'volatility_index_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Lag_2, dtype: float64)\n",
      "Column 'volatility_index_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Lag_3, dtype: float64)\n",
      "Column 'volatility_index_MA_7' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_MA_7, dtype: float64)\n",
      "Column 'volatility_index_MA_30' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_MA_30, dtype: float64)\n",
      "Column 'volatility_index_Std_7' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Std_7, dtype: float64)\n",
      "Column 'volatility_index_Std_30' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Std_30, dtype: float64)\n",
      "Column 'volatility_index_RoC' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_RoC, dtype: float64)\n",
      "Column 'volatility_index_Diff' has values larger than the threshold:\n",
      "Series([], Name: volatility_index_Diff, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Lag_1, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Lag_2, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Lag_3, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_MA_7' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_MA_7, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_MA_30' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_MA_30, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Std_7' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Std_7, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Std_30' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Std_30, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_RoC' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_RoC, dtype: float64)\n",
      "Column 'exchange_rate_usd_eur_Diff' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_eur_Diff, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Lag_1, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Lag_2, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Lag_3, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_MA_7' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_MA_7, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_MA_30' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_MA_30, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Std_7' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Std_7, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Std_30' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Std_30, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_RoC' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_RoC, dtype: float64)\n",
      "Column 'exchange_rate_usd_jpy_Diff' has values larger than the threshold:\n",
      "Series([], Name: exchange_rate_usd_jpy_Diff, dtype: float64)\n",
      "Column 'sp500_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: sp500_Lag_1, dtype: float64)\n",
      "Column 'sp500_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: sp500_Lag_2, dtype: float64)\n",
      "Column 'sp500_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: sp500_Lag_3, dtype: float64)\n",
      "Column 'sp500_MA_7' has values larger than the threshold:\n",
      "Series([], Name: sp500_MA_7, dtype: float64)\n",
      "Column 'sp500_MA_30' has values larger than the threshold:\n",
      "Series([], Name: sp500_MA_30, dtype: float64)\n",
      "Column 'sp500_Std_7' has values larger than the threshold:\n",
      "Series([], Name: sp500_Std_7, dtype: float64)\n",
      "Column 'sp500_Std_30' has values larger than the threshold:\n",
      "Series([], Name: sp500_Std_30, dtype: float64)\n",
      "Column 'sp500_RoC' has values larger than the threshold:\n",
      "Series([], Name: sp500_RoC, dtype: float64)\n",
      "Column 'sp500_Diff' has values larger than the threshold:\n",
      "Series([], Name: sp500_Diff, dtype: float64)\n",
      "Column 'nasdaq_100_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Lag_1, dtype: float64)\n",
      "Column 'nasdaq_100_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Lag_2, dtype: float64)\n",
      "Column 'nasdaq_100_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Lag_3, dtype: float64)\n",
      "Column 'nasdaq_100_MA_7' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_MA_7, dtype: float64)\n",
      "Column 'nasdaq_100_MA_30' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_MA_30, dtype: float64)\n",
      "Column 'nasdaq_100_Std_7' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Std_7, dtype: float64)\n",
      "Column 'nasdaq_100_Std_30' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Std_30, dtype: float64)\n",
      "Column 'nasdaq_100_RoC' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_RoC, dtype: float64)\n",
      "Column 'nasdaq_100_Diff' has values larger than the threshold:\n",
      "Series([], Name: nasdaq_100_Diff, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Lag_1, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Lag_2, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Lag_3, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_MA_7' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_MA_7, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_MA_30' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_MA_30, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Std_7' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Std_7, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Std_30' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Std_30, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_RoC' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_RoC, dtype: float64)\n",
      "Column 'dow_jones_industrial_average_Diff' has values larger than the threshold:\n",
      "Series([], Name: dow_jones_industrial_average_Diff, dtype: float64)\n",
      "Column 'consumer_confidence_index_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Lag_1, dtype: float64)\n",
      "Column 'consumer_confidence_index_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Lag_2, dtype: float64)\n",
      "Column 'consumer_confidence_index_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Lag_3, dtype: float64)\n",
      "Column 'consumer_confidence_index_MA_7' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_MA_7, dtype: float64)\n",
      "Column 'consumer_confidence_index_MA_30' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_MA_30, dtype: float64)\n",
      "Column 'consumer_confidence_index_Std_7' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Std_7, dtype: float64)\n",
      "Column 'consumer_confidence_index_Std_30' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Std_30, dtype: float64)\n",
      "Column 'consumer_confidence_index_RoC' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_RoC, dtype: float64)\n",
      "Column 'consumer_confidence_index_Diff' has values larger than the threshold:\n",
      "Series([], Name: consumer_confidence_index_Diff, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Lag_1, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Lag_2, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Lag_3, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_MA_7' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_MA_7, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_MA_30' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_MA_30, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Std_7' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Std_7, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Std_30' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Std_30, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_RoC' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_RoC, dtype: float64)\n",
      "Column 'vanguard_total_world_stock_etf_Diff' has values larger than the threshold:\n",
      "Series([], Name: vanguard_total_world_stock_etf_Diff, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Lag_1' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Lag_1, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Lag_2' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Lag_2, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Lag_3' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Lag_3, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_MA_7' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_MA_7, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_MA_30' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_MA_30, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Std_7' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Std_7, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Std_30' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Std_30, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_RoC' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_RoC, dtype: float64)\n",
      "Column 'us_treasury_bond_etf_Diff' has values larger than the threshold:\n",
      "Series([], Name: us_treasury_bond_etf_Diff, dtype: float64)\n",
      "Max value after replacement 41198.078125\n",
      "Min value after replacement: 1e-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fill_missing_vals(df):\n",
    "    # Ensure we're only applying the logic to numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Replace any 0s or negative values with extremely small positive integers\n",
    "    numeric_cols = numeric_cols.apply(lambda x: np.where(x <= 0, 1e-10, x))\n",
    "    \n",
    "    # Impute missing values with the following methodologies    \n",
    "    numeric_cols.ffill(inplace=True)\n",
    "    numeric_cols.bfill(inplace=True)\n",
    "    numeric_cols.interpolate(method='linear', inplace=True)\n",
    "    \n",
    "    # Update the original dataframe with the modified numeric columns\n",
    "    df[numeric_cols.columns] = numeric_cols\n",
    "    \n",
    "    return df\n",
    "\n",
    "exo_data = exo_data.copy()\n",
    "\n",
    "# Apply fill_missing_vals only to numeric columns with missing values\n",
    "exo_data = fill_missing_vals(exo_data)\n",
    "\n",
    "print(\"Missing values handled:\")\n",
    "print(exo_data.isna().sum())\n",
    "\n",
    "# We will also put in a check for extremely large values as we are seeing warnings for those below\n",
    "def replace_large_values(df, threshold=1e9):\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Apply logic only to numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Debug print statement to see what and how many values exceed our given threshold.\n",
    "    print(\"Values exceeding the threshold before replacement:\")\n",
    "    for col in numeric_cols.columns:\n",
    "        large_vals = df[col] > threshold\n",
    "        if not large_vals.any():\n",
    "            print(f\"Column '{col}' has values larger than the threshold:\")\n",
    "            print(df[col][large_vals])\n",
    "    \n",
    "    # Replace large values with NaN\n",
    "    df[numeric_cols.columns] = numeric_cols.apply(lambda x: np.where(x > threshold, np.nan, x))\n",
    "    \n",
    "    # Apply the fill_missing_vals logic to impute the modified numeric columns\n",
    "    df = fill_missing_vals(numeric_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "threshold_value = 1e9\n",
    "exo_stock_data = replace_large_values(exo_data, threshold=threshold_value)\n",
    "\n",
    "# Calculate the max and min values only for numeric columns\n",
    "numeric_cols = exo_data.select_dtypes(include = [np.number])\n",
    "\n",
    "print(\"Max value after replacement\", numeric_cols.max().max())\n",
    "print(\"Min value after replacement:\", numeric_cols.min().min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great, now let's look into viewing and handling outliers.  We will first see the appearance of outliers using z_score with a standard std of 3, and then looking at the column feature spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First though we will create a save point and a separate csv of our unscaled secondary stock data, as we will need this in an upcoming notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interest_rates_10yr</th>\n",
       "      <th>gold</th>\n",
       "      <th>copper</th>\n",
       "      <th>platinum</th>\n",
       "      <th>silver</th>\n",
       "      <th>crude_oil</th>\n",
       "      <th>natural_gas</th>\n",
       "      <th>corn</th>\n",
       "      <th>wheat</th>\n",
       "      <th>volatility_index</th>\n",
       "      <th>...</th>\n",
       "      <th>vanguard_total_world_stock_etf_Diff</th>\n",
       "      <th>us_treasury_bond_etf_Lag_1</th>\n",
       "      <th>us_treasury_bond_etf_Lag_2</th>\n",
       "      <th>us_treasury_bond_etf_Lag_3</th>\n",
       "      <th>us_treasury_bond_etf_MA_7</th>\n",
       "      <th>us_treasury_bond_etf_MA_30</th>\n",
       "      <th>us_treasury_bond_etf_Std_7</th>\n",
       "      <th>us_treasury_bond_etf_Std_30</th>\n",
       "      <th>us_treasury_bond_etf_RoC</th>\n",
       "      <th>us_treasury_bond_etf_Diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>2.630</td>\n",
       "      <td>1293.400024</td>\n",
       "      <td>2.8885</td>\n",
       "      <td>825.599976</td>\n",
       "      <td>15.101</td>\n",
       "      <td>58.610001</td>\n",
       "      <td>2.855</td>\n",
       "      <td>361.50</td>\n",
       "      <td>448.25</td>\n",
       "      <td>13.50</td>\n",
       "      <td>...</td>\n",
       "      <td>5.699997e-01</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.967143</td>\n",
       "      <td>25.044</td>\n",
       "      <td>0.097076</td>\n",
       "      <td>0.102876</td>\n",
       "      <td>2.010424e-01</td>\n",
       "      <td>4.999924e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>2.593</td>\n",
       "      <td>1301.800049</td>\n",
       "      <td>2.9035</td>\n",
       "      <td>830.299988</td>\n",
       "      <td>15.253</td>\n",
       "      <td>58.520000</td>\n",
       "      <td>2.795</td>\n",
       "      <td>373.25</td>\n",
       "      <td>462.25</td>\n",
       "      <td>12.88</td>\n",
       "      <td>...</td>\n",
       "      <td>5.699997e-01</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.967143</td>\n",
       "      <td>25.044</td>\n",
       "      <td>0.097076</td>\n",
       "      <td>0.102876</td>\n",
       "      <td>2.010424e-01</td>\n",
       "      <td>4.999924e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>2.602</td>\n",
       "      <td>1300.300049</td>\n",
       "      <td>2.9035</td>\n",
       "      <td>832.599976</td>\n",
       "      <td>15.251</td>\n",
       "      <td>59.090000</td>\n",
       "      <td>2.850</td>\n",
       "      <td>371.50</td>\n",
       "      <td>456.75</td>\n",
       "      <td>13.10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.300018e-01</td>\n",
       "      <td>24.920000</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.967143</td>\n",
       "      <td>25.044</td>\n",
       "      <td>0.097076</td>\n",
       "      <td>0.102876</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>2.614</td>\n",
       "      <td>1305.000000</td>\n",
       "      <td>2.9195</td>\n",
       "      <td>851.200012</td>\n",
       "      <td>15.301</td>\n",
       "      <td>59.029999</td>\n",
       "      <td>2.874</td>\n",
       "      <td>371.25</td>\n",
       "      <td>456.50</td>\n",
       "      <td>13.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000214e-02</td>\n",
       "      <td>24.910000</td>\n",
       "      <td>24.920000</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.967143</td>\n",
       "      <td>25.044</td>\n",
       "      <td>0.097076</td>\n",
       "      <td>0.102876</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>2.535</td>\n",
       "      <td>1300.500000</td>\n",
       "      <td>2.9155</td>\n",
       "      <td>858.200012</td>\n",
       "      <td>15.245</td>\n",
       "      <td>59.830002</td>\n",
       "      <td>2.820</td>\n",
       "      <td>371.50</td>\n",
       "      <td>464.75</td>\n",
       "      <td>13.91</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>24.910000</td>\n",
       "      <td>24.920000</td>\n",
       "      <td>24.967143</td>\n",
       "      <td>25.044</td>\n",
       "      <td>0.097076</td>\n",
       "      <td>0.102876</td>\n",
       "      <td>4.417695e-01</td>\n",
       "      <td>1.100006e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            interest_rates_10yr         gold  copper    platinum  silver  \\\n",
       "Date                                                                       \n",
       "2019-03-14                2.630  1293.400024  2.8885  825.599976  15.101   \n",
       "2019-03-15                2.593  1301.800049  2.9035  830.299988  15.253   \n",
       "2019-03-18                2.602  1300.300049  2.9035  832.599976  15.251   \n",
       "2019-03-19                2.614  1305.000000  2.9195  851.200012  15.301   \n",
       "2019-03-20                2.535  1300.500000  2.9155  858.200012  15.245   \n",
       "\n",
       "            crude_oil  natural_gas    corn   wheat  volatility_index  ...  \\\n",
       "Date                                                                  ...   \n",
       "2019-03-14  58.610001        2.855  361.50  448.25             13.50  ...   \n",
       "2019-03-15  58.520000        2.795  373.25  462.25             12.88  ...   \n",
       "2019-03-18  59.090000        2.850  371.50  456.75             13.10  ...   \n",
       "2019-03-19  59.029999        2.874  371.25  456.50             13.56  ...   \n",
       "2019-03-20  59.830002        2.820  371.50  464.75             13.91  ...   \n",
       "\n",
       "            vanguard_total_world_stock_etf_Diff  us_treasury_bond_etf_Lag_1  \\\n",
       "Date                                                                          \n",
       "2019-03-14                         5.699997e-01                   24.870001   \n",
       "2019-03-15                         5.699997e-01                   24.870001   \n",
       "2019-03-18                         3.300018e-01                   24.920000   \n",
       "2019-03-19                         1.000214e-02                   24.910000   \n",
       "2019-03-20                         1.000000e-10                   24.900000   \n",
       "\n",
       "            us_treasury_bond_etf_Lag_2  us_treasury_bond_etf_Lag_3  \\\n",
       "Date                                                                 \n",
       "2019-03-14                   24.870001                   24.870001   \n",
       "2019-03-15                   24.870001                   24.870001   \n",
       "2019-03-18                   24.870001                   24.870001   \n",
       "2019-03-19                   24.920000                   24.870001   \n",
       "2019-03-20                   24.910000                   24.920000   \n",
       "\n",
       "            us_treasury_bond_etf_MA_7  us_treasury_bond_etf_MA_30  \\\n",
       "Date                                                                \n",
       "2019-03-14                  24.967143                      25.044   \n",
       "2019-03-15                  24.967143                      25.044   \n",
       "2019-03-18                  24.967143                      25.044   \n",
       "2019-03-19                  24.967143                      25.044   \n",
       "2019-03-20                  24.967143                      25.044   \n",
       "\n",
       "            us_treasury_bond_etf_Std_7  us_treasury_bond_etf_Std_30  \\\n",
       "Date                                                                  \n",
       "2019-03-14                    0.097076                     0.102876   \n",
       "2019-03-15                    0.097076                     0.102876   \n",
       "2019-03-18                    0.097076                     0.102876   \n",
       "2019-03-19                    0.097076                     0.102876   \n",
       "2019-03-20                    0.097076                     0.102876   \n",
       "\n",
       "            us_treasury_bond_etf_RoC  us_treasury_bond_etf_Diff  \n",
       "Date                                                             \n",
       "2019-03-14              2.010424e-01               4.999924e-02  \n",
       "2019-03-15              2.010424e-01               4.999924e-02  \n",
       "2019-03-18              1.000000e-10               1.000000e-10  \n",
       "2019-03-19              1.000000e-10               1.000000e-10  \n",
       "2019-03-20              4.417695e-01               1.100006e-01  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exo_data.to_csv(os.path.join(project_root, 'data', 'exo_data_unscaled.csv'), index = True)\n",
    "\n",
    "exo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_rates_10yr             0\n",
      "gold                            0\n",
      "copper                          0\n",
      "platinum                        8\n",
      "silver                          0\n",
      "                               ..\n",
      "us_treasury_bond_etf_MA_30      0\n",
      "us_treasury_bond_etf_Std_7     23\n",
      "us_treasury_bond_etf_Std_30    20\n",
      "us_treasury_bond_etf_RoC       31\n",
      "us_treasury_bond_etf_Diff      28\n",
      "Length: 180, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numeric_cols = exo_data.select_dtypes(include = [np.number])\n",
    "z_scores = np.abs(stats.zscore(numeric_cols))\n",
    "threshold = 3 # Common threshold starting std modifier\n",
    "\n",
    "outliers = (z_scores > threshold)\n",
    "\n",
    "print(outliers.sum(axis = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is great, just using a standard zscore we are able to isolate the vast majority of potential outliers.  No further activity is needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the last phase of our preprocessing notebook we can now move on to scaling our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            interest_rates_10yr      gold    copper  platinum    silver  \\\n",
      "Date                                                                      \n",
      "2019-03-14             0.109020 -2.157252 -1.047962 -1.293853 -1.757015   \n",
      "2019-03-15             0.080561 -2.122515 -1.026817 -1.248374 -1.719625   \n",
      "2019-03-18             0.087483 -2.128718 -1.026817 -1.226118 -1.720117   \n",
      "2019-03-19             0.096713 -2.109282 -1.004263 -1.046136 -1.707818   \n",
      "2019-03-20             0.035949 -2.127891 -1.009902 -0.978401 -1.721593   \n",
      "\n",
      "            crude_oil  natural_gas      corn     wheat  volatility_index  ...  \\\n",
      "Date                                                                      ...   \n",
      "2019-03-14  -0.529975    -0.294268 -1.127038 -1.329873         -0.877685  ...   \n",
      "2019-03-15  -0.534518    -0.327440 -1.039349 -1.239960         -0.953523  ...   \n",
      "2019-03-18  -0.505745    -0.297032 -1.052409 -1.275283         -0.926613  ...   \n",
      "2019-03-19  -0.508773    -0.283763 -1.054275 -1.276889         -0.870346  ...   \n",
      "2019-03-20  -0.468389    -0.313618 -1.052409 -1.223904         -0.827535  ...   \n",
      "\n",
      "            vanguard_total_world_stock_etf_Diff  us_treasury_bond_etf_Lag_1  \\\n",
      "Date                                                                          \n",
      "2019-03-14                             0.366222                   -0.082238   \n",
      "2019-03-15                             0.366222                   -0.082238   \n",
      "2019-03-18                            -0.054965                   -0.057053   \n",
      "2019-03-19                            -0.616551                   -0.062090   \n",
      "2019-03-20                            -0.634105                   -0.067127   \n",
      "\n",
      "            us_treasury_bond_etf_Lag_2  us_treasury_bond_etf_Lag_3  \\\n",
      "Date                                                                 \n",
      "2019-03-14                   -0.082830                   -0.083454   \n",
      "2019-03-15                   -0.082830                   -0.083454   \n",
      "2019-03-18                   -0.082830                   -0.083454   \n",
      "2019-03-19                   -0.057638                   -0.083454   \n",
      "2019-03-20                   -0.062677                   -0.058254   \n",
      "\n",
      "            us_treasury_bond_etf_MA_7  us_treasury_bond_etf_MA_30  \\\n",
      "Date                                                                \n",
      "2019-03-14                  -0.034733                   -0.004451   \n",
      "2019-03-15                  -0.034733                   -0.004451   \n",
      "2019-03-18                  -0.034733                   -0.004451   \n",
      "2019-03-19                  -0.034733                   -0.004451   \n",
      "2019-03-20                  -0.034733                   -0.004451   \n",
      "\n",
      "            us_treasury_bond_etf_Std_7  us_treasury_bond_etf_Std_30  \\\n",
      "Date                                                                  \n",
      "2019-03-14                    0.161541                    -0.814098   \n",
      "2019-03-15                    0.161541                    -0.814098   \n",
      "2019-03-18                    0.161541                    -0.814098   \n",
      "2019-03-19                    0.161541                    -0.814098   \n",
      "2019-03-20                    0.161541                    -0.814098   \n",
      "\n",
      "            us_treasury_bond_etf_RoC  us_treasury_bond_etf_Diff  \n",
      "Date                                                             \n",
      "2019-03-14                  0.283899                   0.289470  \n",
      "2019-03-15                  0.283899                   0.289470  \n",
      "2019-03-18                 -0.588514                  -0.587270  \n",
      "2019-03-19                 -0.588514                  -0.587270  \n",
      "2019-03-20                  1.328522                   1.341598  \n",
      "\n",
      "[5 rows x 180 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initiate the scaler and transform the data.\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(exo_data)\n",
    "exo_scaled = pd.DataFrame(scaled_data, index = exo_data.index, columns = exo_data.columns)\n",
    "\n",
    "print(exo_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This looks good, let's save it so we can use later on in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_scaled.to_csv(os.path.join(project_root, 'data', 'exo_data_preprocessed.csv'), index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary we have taken our raw secondary stock data that we generated from our generate_secondary_stocks.py script and have preprocessed it for further purposes.  We have created a 3-day lag window and our custom features, addressed any missing values and imputed accordingly, and looked at outliers a bit and making sure that we established normal distribution in at least a few of our features for good measure.  We ended this notebook by scaling our new dataframe using StandardScaler so that it is ready to go upon next use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Strat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
