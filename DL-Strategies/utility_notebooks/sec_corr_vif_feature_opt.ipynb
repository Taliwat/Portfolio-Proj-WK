{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Secondary Stock Data - Feature Filtering and Optimization Using Correlation Matrix and VIF Notebook**\n",
    "## In this notebook we will take our secondary stock data that has been previously preprocessed in sec_stock_preprocessing.ipynb and apply a method called VIF (Variance Inflation Factor) as well as a triangular correlation matrix to optimize our list of lagged features based on our target of the Closing Price for our data.  In other parts of this project it has been shown that there is extremely high collinearity with a lot of our features with our target, creating a lot of data leakage.  This will really skew our model results, so it is something we need to look at now before we get too far along.  Once we optimize our features our models will be much more efficient, as well as using this refined list of features to generate our secondary stocks for better cointegration tests at later parts of the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As usual let's start by bringing in the libraries and logic necessary for reading in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's read in our notebook and take a look; we will need the scaled version of our preprocessed secondary stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Close_sec  Volume_sec  Open_sec  High_sec   Low_sec   SMA_sec  \\\n",
      "Date                                                                        \n",
      "2019-03-14  -0.259231   -0.558310 -0.255721 -0.258880 -0.255947 -0.249447   \n",
      "2019-03-15  -0.260650    0.087049 -0.259482 -0.260749 -0.257680 -0.249447   \n",
      "2019-03-18  -0.267337   -0.044652 -0.260699 -0.263240 -0.265112 -0.249447   \n",
      "2019-03-19  -0.266211   -0.464214 -0.266780 -0.268624 -0.264815 -0.249447   \n",
      "2019-03-20  -0.264162   -0.246529 -0.263469 -0.262818 -0.261669 -0.249447   \n",
      "\n",
      "             EMA_sec   RSI_sec   BBM_sec   BBU_sec  ...  \\\n",
      "Date                                                ...   \n",
      "2019-03-14 -0.258458  0.678803 -0.249447 -0.254554  ...   \n",
      "2019-03-15 -0.258515  0.678803 -0.249447 -0.254554  ...   \n",
      "2019-03-18 -0.258838  0.678803 -0.249447 -0.254554  ...   \n",
      "2019-03-19 -0.259103  0.678803 -0.249447 -0.254554  ...   \n",
      "2019-03-20 -0.259275  0.678803 -0.249447 -0.254554  ...   \n",
      "\n",
      "            Momentum_7_Lag_Std_1_3_sec  Momentum_30_Lag_Avg_1_3_sec  \\\n",
      "Date                                                                  \n",
      "2019-03-14                   -0.403697                    -0.015335   \n",
      "2019-03-15                   -0.403697                    -0.015335   \n",
      "2019-03-18                   -0.403697                    -0.015335   \n",
      "2019-03-19                   -0.403697                    -0.015335   \n",
      "2019-03-20                   -0.403697                    -0.015335   \n",
      "\n",
      "            Momentum_30_Lag_Std_1_3_sec  Momentum_50_Lag_Avg_1_3_sec  \\\n",
      "Date                                                                   \n",
      "2019-03-14                    -0.401493                     0.117021   \n",
      "2019-03-15                    -0.401493                     0.117021   \n",
      "2019-03-18                    -0.401493                     0.117021   \n",
      "2019-03-19                    -0.401493                     0.117021   \n",
      "2019-03-20                    -0.401493                     0.117021   \n",
      "\n",
      "            Momentum_50_Lag_Std_1_3_sec  OBV_Lag_Avg_1_3_sec  \\\n",
      "Date                                                           \n",
      "2019-03-14                    -0.397774            -0.268290   \n",
      "2019-03-15                    -0.397774            -0.268290   \n",
      "2019-03-18                    -0.397774            -0.272165   \n",
      "2019-03-19                    -0.397774            -0.275629   \n",
      "2019-03-20                    -0.397774            -0.279136   \n",
      "\n",
      "            OBV_Lag_Std_1_3_sec  Diff_Close_EMA_sec  Ratio_Close_EMA_sec  \\\n",
      "Date                                                                       \n",
      "2019-03-14            -0.200418           -0.094854            -0.119642   \n",
      "2019-03-15            -0.200418           -0.114817            -0.196878   \n",
      "2019-03-18            -0.200418           -0.208147            -0.558578   \n",
      "2019-03-19            -0.163334           -0.187860            -0.480392   \n",
      "2019-03-20            -0.249800           -0.155377            -0.354572   \n",
      "\n",
      "            ticker  \n",
      "Date                \n",
      "2019-03-14     RMD  \n",
      "2019-03-15     RMD  \n",
      "2019-03-18     RMD  \n",
      "2019-03-19     RMD  \n",
      "2019-03-20     RMD  \n",
      "\n",
      "[5 rows x 153 columns]\n",
      "(266665, 153)\n"
     ]
    }
   ],
   "source": [
    "# Now let's access the main core_stock_data.csv file\n",
    "csv_path = os.path.join(project_root, 'data', 'sec_stock_preprocessed.csv')\n",
    "sec_stock_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "print(sec_stock_data.head())\n",
    "print(sec_stock_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to first set up the Correlation Matrix, and will start with a threshold of 0.95.  Doing the correlation matrix first will allow us to filter out the first batch of features which will set us up nicely for our VIF calculations.  First though let's drop our ticker column as it is not in the right format to be processed by the following calculations.  We also need to introduce our target in our Close Price (Close_sec), as all of the correlations will be based on relationships with this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features that correlate well with the target (Close_sec):\n",
      "Close_sec                      1.000000\n",
      "Volume_sec                     0.448197\n",
      "Open_sec                       0.999826\n",
      "High_sec                       0.999914\n",
      "Low_sec                        0.999921\n",
      "                                 ...   \n",
      "Momentum_30_Lag_Avg_1_3_sec    0.287041\n",
      "Momentum_30_Lag_Std_1_3_sec    0.716781\n",
      "Momentum_50_Lag_Avg_1_3_sec    0.358614\n",
      "Momentum_50_Lag_Std_1_3_sec    0.721166\n",
      "Diff_Close_EMA_sec             0.353725\n",
      "Name: Close_sec, Length: 85, dtype: float64\n",
      "Number of remaining features after correlation matrix filtering:\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# Setting up our target for the correlation\n",
    "target = 'Close_sec'\n",
    "\n",
    "# Dropping ticker column for now.\n",
    "sec_stock_data = sec_stock_data.drop(columns = ['ticker'])\n",
    "\n",
    "# Calculate the correlation of each feature with the target\n",
    "corr_with_target = sec_stock_data.corr()[target].abs()\n",
    "\n",
    "# Set a correlation threshold with the target \n",
    "corr_threshold_with_target = 0.25\n",
    "\n",
    "# Filter features that have a decent correlation with the target\n",
    "selected_features = corr_with_target[corr_with_target > corr_threshold_with_target].index\n",
    "\n",
    "# Print correlation results for selected features\n",
    "print(f\"Features that correlate well with the target ({target}):\\n{corr_with_target[selected_features]}\")\n",
    "\n",
    "\n",
    "# Set up the correlation matrix for the selected features (after filtering by target correlation)\n",
    "corr_matrix = sec_stock_data[selected_features].corr().abs()\n",
    "\n",
    "# Set the pairwise correlation threshold\n",
    "corr_threshold = 0.85\n",
    "\n",
    "# Set up a triangular matrix to identify features with high pairwise correlation\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "selected_features = corr_with_target[corr_with_target > corr_threshold_with_target].index\n",
    "\n",
    "selected_features = [feature for feature in selected_features if feature != target]\n",
    "\n",
    "\n",
    "print(f\"Number of remaining features after correlation matrix filtering:\\n{len(selected_features)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good start, we filtered down to 59 features from 151 so we did a nice job of not over-filtering.  That will give our VIF (Variance Inflation Factor) a good pool to work with and we can manually step that threshold and adjust it until we find the set of features we like.  Let's get that VIF Calculation up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanm\\Desktop\\Portfolio-Proj-WK\\DL-Strategies\\DL-Strat\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final selected features with VIF < 30:\n",
      "0                      Volume_sec\n",
      "70           MACD_Lag_Std_1_3_sec\n",
      "72    MACD_Signal_Lag_Std_1_3_sec\n",
      "73      MACD_Hist_Lag_Std_1_3_sec\n",
      "75         ATR_14_Lag_Std_1_3_sec\n",
      "76     Momentum_1_Lag_Std_1_3_sec\n",
      "77     Momentum_3_Lag_Std_1_3_sec\n",
      "78     Momentum_7_Lag_Std_1_3_sec\n",
      "80    Momentum_30_Lag_Std_1_3_sec\n",
      "82    Momentum_50_Lag_Std_1_3_sec\n",
      "Name: Feature, dtype: object\n",
      "Number of final selected features: 10\n",
      "VIF values of selected features:\n",
      "                        Feature       VIF\n",
      "0                    Volume_sec  1.265354\n",
      "70         MACD_Lag_Std_1_3_sec  7.740249\n",
      "72  MACD_Signal_Lag_Std_1_3_sec  5.483497\n",
      "73    MACD_Hist_Lag_Std_1_3_sec  6.205859\n",
      "75       ATR_14_Lag_Std_1_3_sec  6.016526\n",
      "76   Momentum_1_Lag_Std_1_3_sec  6.654860\n",
      "77   Momentum_3_Lag_Std_1_3_sec  4.478609\n",
      "78   Momentum_7_Lag_Std_1_3_sec  5.042563\n",
      "80  Momentum_30_Lag_Std_1_3_sec  4.168060\n",
      "82  Momentum_50_Lag_Std_1_3_sec  4.206210\n"
     ]
    }
   ],
   "source": [
    "# Let's set up our VIF Calculation here.  we will make it as a function so we can tune our threshold as needed.\n",
    "\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = X.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    \n",
    "    return vif_data\n",
    "\n",
    "# Filter data for VIF calculation\n",
    "sec_data_for_vif = sec_stock_data[selected_features]\n",
    "\n",
    "# Calculate VIF for the remaining features\n",
    "vif_result = calculate_vif(sec_data_for_vif)\n",
    "\n",
    "# Set a VIF threshold \n",
    "vif_threshold = 30\n",
    "\n",
    "# Filter features that have VIF below the threshold\n",
    "filtered_vif_result = vif_result[vif_result['VIF'] < vif_threshold]\n",
    "\n",
    "final_features = filtered_vif_result['Feature']\n",
    "\n",
    "# Print final selected features\n",
    "print(f\"Final selected features with VIF < {vif_threshold}:\\n{final_features}\")\n",
    "print(f\"Number of final selected features: {len(final_features)}\")\n",
    "\n",
    "print(f\"VIF values of selected features:\\n{filtered_vif_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have a final list of 10 features, down from our original 151, that have been optimized according to our target according to their rate of collinearity.  Due to time restrictions we will just manually implement these in various areas in our project for now.  Later as time allows we will explore other methodologies (RFECV/Ridge, Boruta with RandomForest, etc.) to see if the results differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook we used correlation matrix and VIF (Variance Inflation Factor) to filter down and score our features against our target feature in Closing Price (Close_Sec).  We ended up with 10 features that we will manually implement where needed now in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Strat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
