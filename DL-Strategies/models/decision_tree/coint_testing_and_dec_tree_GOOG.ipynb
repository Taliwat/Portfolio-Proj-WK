{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cointegration Testing Analysis and Decision Tree Modeling for the GOOG Stock**\n",
    "## In this notebook we start to bring things together from other sections of the project.  We combine our analyzed and preprocessed core_stock_data with our preprocessed and dynamically generated secondary stock data to perform cointegration testing to see if we can create cointegrated pairs, which we will add as separate features to use for our Decision Tree model.  We will also merge in our exogenous data for this Decision Tree model, data we haven't talked about a whole lot yet.  This data will aid in adding in another layer to the stock chains we want to create at the end output at the end of the Decision Tree based on their relationships.  We will use the output of the Decision Tree to read into how the stocks work with each other so we can start to formulate our trading strategy in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As with our other notebooks let's read in our data to use for this.  This time let's take a good look at the shape of our dataframes since we will plan on merging them later on in the notebook.  It will be important that the dataframes match in size so that modeling works for our Decision Trees.  If they mismatch we will have to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Close_core  Volume_core  Open_core  High_core   Low_core  \\\n",
      "Date                                                                   \n",
      "2019-03-14   45.932499   94318000.0  45.974998  46.025002  45.639999   \n",
      "2019-03-15   46.529999  156171600.0  46.212502  46.832500  45.935001   \n",
      "2019-03-18   47.005001  104879200.0  46.450001  47.097500  46.447498   \n",
      "2019-03-19   46.632500  126585600.0  47.087502  47.247501  46.480000   \n",
      "2019-03-20   47.040001  124140800.0  46.557499  47.372501  46.182499   \n",
      "\n",
      "            SMA_core   EMA_core   RSI_core  BBM_core   BBU_core  ...  \\\n",
      "Date                                                             ...   \n",
      "2019-03-14  41.35925  42.219051  75.741602  41.35925  46.695085  ...   \n",
      "2019-03-15  41.50025  42.388107  76.985910  41.50025  47.003365  ...   \n",
      "2019-03-18  41.72940  42.569162  78.724282  41.72940  47.174667  ...   \n",
      "2019-03-19  41.92075  42.728509  73.527018  41.92075  47.369412  ...   \n",
      "2019-03-20  42.12190  42.897587  80.396901  42.12190  47.569044  ...   \n",
      "\n",
      "            Momentum_7_Lag_Avg_1_3_core  Momentum_7_Lag_Std_1_3_core  \\\n",
      "Date                                                                   \n",
      "2019-03-14                     2.049999                     0.601040   \n",
      "2019-03-15                     2.049999                     0.601040   \n",
      "2019-03-18                     2.474998                     0.601040   \n",
      "2019-03-19                     2.943333                     0.915770   \n",
      "2019-03-20                     3.394999                     0.490078   \n",
      "\n",
      "            Momentum_30_Lag_Avg_1_3_core  Momentum_30_Lag_Std_1_3_core  \\\n",
      "Date                                                                     \n",
      "2019-03-14                      4.619999                      0.212131   \n",
      "2019-03-15                      4.619999                      0.212131   \n",
      "2019-03-18                      4.769999                      0.212131   \n",
      "2019-03-19                      4.971666                      0.380143   \n",
      "2019-03-20                      4.704999                      0.799484   \n",
      "\n",
      "            Momentum_50_Lag_Avg_1_3_core  Momentum_50_Lag_Std_1_3_core  \\\n",
      "Date                                                                     \n",
      "2019-03-14                      7.049999                  1.000000e-10   \n",
      "2019-03-15                      7.049999                  1.000000e-10   \n",
      "2019-03-18                      7.049999                  1.000000e-10   \n",
      "2019-03-19                      8.519166                  2.544672e+00   \n",
      "2019-03-20                      9.358334                  2.211183e+00   \n",
      "\n",
      "            OBV_Lag_Avg_1_3_core  OBV_Lag_Std_1_3_core  Diff_Close_EMA_core  \\\n",
      "Date                                                                          \n",
      "2019-03-14          1.592191e+09          1.104300e+08             3.713448   \n",
      "2019-03-15          1.592191e+09          1.104300e+08             4.141891   \n",
      "2019-03-18          1.670277e+09          1.104300e+08             4.435839   \n",
      "2019-03-19          1.731265e+09          1.313626e+08             3.903991   \n",
      "2019-03-20          1.776087e+09          6.769383e+07             4.142414   \n",
      "\n",
      "            Ratio_Close_EMA_core  \n",
      "Date                              \n",
      "2019-03-14              1.087957  \n",
      "2019-03-15              1.097714  \n",
      "2019-03-18              1.104203  \n",
      "2019-03-19              1.091367  \n",
      "2019-03-20              1.096565  \n",
      "\n",
      "[5 rows x 153 columns]\n",
      "(13650, 153)\n",
      "           ticker   Close_sec  Volume_sec    Open_sec    High_sec     Low_sec  \\\n",
      "Date                                                                            \n",
      "2019-03-14    RMD  101.000000    972500.0  102.570000  102.570000  100.959999   \n",
      "2019-03-15    RMD  100.370003   2279400.0  100.900002  101.730003  100.199997   \n",
      "2019-03-18    RMD   97.400002   1915700.0  100.360001  100.610001   96.940002   \n",
      "2019-03-19    RMD   97.900002   1101100.0   97.660004   98.190002   97.070000   \n",
      "2019-03-20    RMD   98.809998   1467600.0   99.129997  100.800003   98.449997   \n",
      "\n",
      "             SMA_sec     EMA_sec    RSI_sec   BBM_sec  ...  \\\n",
      "Date                                                   ...   \n",
      "2019-03-14  104.9046  101.000000  63.818062  104.9046  ...   \n",
      "2019-03-15  104.9046  100.975294  63.818062  104.9046  ...   \n",
      "2019-03-18  104.9046  100.835087  63.818062  104.9046  ...   \n",
      "2019-03-19  104.9046  100.719985  63.818062  104.9046  ...   \n",
      "2019-03-20  104.9046  100.645084  63.818062  104.9046  ...   \n",
      "\n",
      "            Momentum_7_Lag_Avg_1_3_sec  Momentum_7_Lag_Std_1_3_sec  \\\n",
      "Date                                                                 \n",
      "2019-03-14                    0.419998                1.000000e-10   \n",
      "2019-03-15                    0.419998                1.000000e-10   \n",
      "2019-03-18                    0.419998                1.000000e-10   \n",
      "2019-03-19                    0.419998                1.000000e-10   \n",
      "2019-03-20                    0.419998                1.000000e-10   \n",
      "\n",
      "            Momentum_30_Lag_Avg_1_3_sec  Momentum_30_Lag_Std_1_3_sec  \\\n",
      "Date                                                                   \n",
      "2019-03-14                     2.940002                 1.000000e-10   \n",
      "2019-03-15                     2.940002                 1.000000e-10   \n",
      "2019-03-18                     2.940002                 1.000000e-10   \n",
      "2019-03-19                     2.940002                 1.000000e-10   \n",
      "2019-03-20                     2.940002                 1.000000e-10   \n",
      "\n",
      "            Momentum_50_Lag_Avg_1_3_sec  Momentum_50_Lag_Std_1_3_sec  \\\n",
      "Date                                                                   \n",
      "2019-03-14                    12.889999                 1.000000e-10   \n",
      "2019-03-15                    12.889999                 1.000000e-10   \n",
      "2019-03-18                    12.889999                 1.000000e-10   \n",
      "2019-03-19                    12.889999                 1.000000e-10   \n",
      "2019-03-20                    12.889999                 1.000000e-10   \n",
      "\n",
      "            OBV_Lag_Avg_1_3_sec  OBV_Lag_Std_1_3_sec  Diff_Close_EMA_sec  \\\n",
      "Date                                                                       \n",
      "2019-03-14         1.000000e-10         1.611779e+06        1.000000e-10   \n",
      "2019-03-15         1.000000e-10         1.611779e+06        1.000000e-10   \n",
      "2019-03-18         1.000000e-10         1.611779e+06        1.000000e-10   \n",
      "2019-03-19         1.000000e-10         2.100176e+06        1.000000e-10   \n",
      "2019-03-20         1.000000e-10         9.614140e+05        1.000000e-10   \n",
      "\n",
      "            Ratio_Close_EMA_sec  \n",
      "Date                             \n",
      "2019-03-14             1.000000  \n",
      "2019-03-15             0.994006  \n",
      "2019-03-18             0.965934  \n",
      "2019-03-19             0.972002  \n",
      "2019-03-20             0.981767  \n",
      "\n",
      "[5 rows x 153 columns]\n",
      "(266665, 153)\n",
      "            interest_rates_10yr         gold  copper    platinum  silver  \\\n",
      "Date                                                                       \n",
      "2019-03-14                2.630  1293.400024  2.8885  825.599976  15.101   \n",
      "2019-03-15                2.593  1301.800049  2.9035  830.299988  15.253   \n",
      "2019-03-18                2.602  1300.300049  2.9035  832.599976  15.251   \n",
      "2019-03-19                2.614  1305.000000  2.9195  851.200012  15.301   \n",
      "2019-03-20                2.535  1300.500000  2.9155  858.200012  15.245   \n",
      "\n",
      "            crude_oil  natural_gas    corn   wheat  volatility_index  ...  \\\n",
      "Date                                                                  ...   \n",
      "2019-03-14  58.610001        2.855  361.50  448.25             13.50  ...   \n",
      "2019-03-15  58.520000        2.795  373.25  462.25             12.88  ...   \n",
      "2019-03-18  59.090000        2.850  371.50  456.75             13.10  ...   \n",
      "2019-03-19  59.029999        2.874  371.25  456.50             13.56  ...   \n",
      "2019-03-20  59.830002        2.820  371.50  464.75             13.91  ...   \n",
      "\n",
      "            vanguard_total_world_stock_etf_Diff  us_treasury_bond_etf_Lag_1  \\\n",
      "Date                                                                          \n",
      "2019-03-14                         5.699997e-01                   24.870001   \n",
      "2019-03-15                         5.699997e-01                   24.870001   \n",
      "2019-03-18                         3.300018e-01                   24.920000   \n",
      "2019-03-19                         1.000214e-02                   24.910000   \n",
      "2019-03-20                         1.000000e-10                   24.900000   \n",
      "\n",
      "            us_treasury_bond_etf_Lag_2  us_treasury_bond_etf_Lag_3  \\\n",
      "Date                                                                 \n",
      "2019-03-14                   24.870001                   24.870001   \n",
      "2019-03-15                   24.870001                   24.870001   \n",
      "2019-03-18                   24.870001                   24.870001   \n",
      "2019-03-19                   24.920000                   24.870001   \n",
      "2019-03-20                   24.910000                   24.920000   \n",
      "\n",
      "            us_treasury_bond_etf_MA_7  us_treasury_bond_etf_MA_30  \\\n",
      "Date                                                                \n",
      "2019-03-14                  24.967143                      25.044   \n",
      "2019-03-15                  24.967143                      25.044   \n",
      "2019-03-18                  24.967143                      25.044   \n",
      "2019-03-19                  24.967143                      25.044   \n",
      "2019-03-20                  24.967143                      25.044   \n",
      "\n",
      "            us_treasury_bond_etf_Std_7  us_treasury_bond_etf_Std_30  \\\n",
      "Date                                                                  \n",
      "2019-03-14                    0.097076                     0.102876   \n",
      "2019-03-15                    0.097076                     0.102876   \n",
      "2019-03-18                    0.097076                     0.102876   \n",
      "2019-03-19                    0.097076                     0.102876   \n",
      "2019-03-20                    0.097076                     0.102876   \n",
      "\n",
      "            us_treasury_bond_etf_RoC  us_treasury_bond_etf_Diff  \n",
      "Date                                                             \n",
      "2019-03-14              2.010424e-01               4.999924e-02  \n",
      "2019-03-15              2.010424e-01               4.999924e-02  \n",
      "2019-03-18              1.000000e-10               1.000000e-10  \n",
      "2019-03-19              1.000000e-10               1.000000e-10  \n",
      "2019-03-20              4.417695e-01               1.100006e-01  \n",
      "\n",
      "[5 rows x 180 columns]\n",
      "(1415, 180)\n"
     ]
    }
   ],
   "source": [
    "# Now let's access the main core_stock_data.csv file, as well as the secondary stock data and exo_data to all be used in this notebook!\n",
    "csv_path = os.path.join(project_root, 'data', 'core_stock_unscaled.csv')\n",
    "core_stock_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "print(core_stock_data.head())\n",
    "print(core_stock_data.shape)\n",
    "\n",
    "csv_path = os.path.join(project_root, 'data', 'secondary_stocks_gen_filtered.csv')\n",
    "sec_stock_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "print(sec_stock_data.head())\n",
    "print(sec_stock_data.shape)\n",
    "\n",
    "csv_path = os.path.join(project_root, 'data', 'exo_data_unscaled.csv')\n",
    "exo_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "\n",
    "print(exo_data.head())\n",
    "print(exo_data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-04-19', '2019-05-27', '2019-07-04', '2019-09-02',\n",
      "               '2019-11-28', '2019-12-25', '2020-01-01', '2020-01-20',\n",
      "               '2020-02-17', '2020-04-10', '2020-05-25', '2020-07-03',\n",
      "               '2020-09-07', '2020-11-26', '2020-12-25', '2021-01-01',\n",
      "               '2021-01-18', '2021-02-15', '2021-04-02', '2021-05-31',\n",
      "               '2021-07-05', '2021-09-06', '2021-11-25', '2021-12-24',\n",
      "               '2022-01-17', '2022-02-21', '2022-04-15', '2022-05-30',\n",
      "               '2022-06-20', '2022-07-04', '2022-09-05', '2022-11-24',\n",
      "               '2022-12-26', '2023-01-02', '2023-01-16', '2023-02-20',\n",
      "               '2023-04-07', '2023-05-29', '2023-06-19', '2023-07-04',\n",
      "               '2023-09-04', '2023-11-23', '2023-12-25', '2024-01-01',\n",
      "               '2024-01-15', '2024-02-19', '2024-03-29', '2024-05-27',\n",
      "               '2024-06-19', '2024-07-04'],\n",
      "              dtype='datetime64[ns]', name='Date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "exo_only_dates = exo_data.index.difference(core_stock_data.index)\n",
    "print(exo_only_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's grab just our GOOG stock for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1365, 153)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will want to preserve this version of our data since we will be doing our models later on, and this variable will serve as our target.\n",
    "goog_data_orig = core_stock_data[core_stock_data['Ticker'] == 'GOOG']\n",
    "\n",
    "\n",
    "goog_data_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now looking back at exo_data I see a mismatch in our data.  Sec_stock_data is a different story as we still need it as it for now, and we will look at it later.  For now though let's look into exo_data and fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-04-19', '2019-05-27', '2019-07-04', '2019-09-02',\n",
      "               '2019-11-28', '2019-12-25', '2020-01-01', '2020-01-20',\n",
      "               '2020-02-17', '2020-04-10', '2020-05-25', '2020-07-03',\n",
      "               '2020-09-07', '2020-11-26', '2020-12-25', '2021-01-01',\n",
      "               '2021-01-18', '2021-02-15', '2021-04-02', '2021-05-31',\n",
      "               '2021-07-05', '2021-09-06', '2021-11-25', '2021-12-24',\n",
      "               '2022-01-17', '2022-02-21', '2022-04-15', '2022-05-30',\n",
      "               '2022-06-20', '2022-07-04', '2022-09-05', '2022-11-24',\n",
      "               '2022-12-26', '2023-01-02', '2023-01-16', '2023-02-20',\n",
      "               '2023-04-07', '2023-05-29', '2023-06-19', '2023-07-04',\n",
      "               '2023-09-04', '2023-11-23', '2023-12-25', '2024-01-01',\n",
      "               '2024-01-15', '2024-02-19', '2024-03-29', '2024-05-27',\n",
      "               '2024-06-19', '2024-07-04'],\n",
      "              dtype='datetime64[ns]', name='Date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "exo_only_dates = exo_data.index.difference(goog_data_orig.index)\n",
    "print(exo_only_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 50 different dates and values that our exo_data has that our core target data does not.  Upon first glance they appear to be related to holidays.  Let's remove these values from exo_data so that the indices align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exo_data shape after dropping extra values: (1365, 180)\n"
     ]
    }
   ],
   "source": [
    "exo_data_cleaned = exo_data.drop(index = exo_only_dates)\n",
    "\n",
    "# Now re-check shape\n",
    "print(f\"Exo_data shape after dropping extra values: {exo_data_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready.  We have our core stock and our secondary stock data.  Before we do the cointegration tests we will perform a ADF (Augmented Dickey-Fuller) test to check for stationarity.  This will help our cointegration results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Statistic: -0.8661057893662136\n",
      "p-value: 0.7989293021990264\n",
      "Critical Values: {'1%': -3.435163869552687, '5%': -2.863665960737661, '10%': -2.567901861810129}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform ADF Test on the 'Close' price column\n",
    "adf_result = adfuller(goog_data_orig['Close_core'])\n",
    "\n",
    "# Display ADF test results\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "print('Critical Values:', adf_result[4])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's break down our results.  The ADF Statistic as a negative number is expected and a higher negative number here suggests stronger evidence against our hypothesis that the data is non-stationary.  The output here is fairly large which means the data is very likely to be stationary.  For the p value a value of 0 can occur when the ADF statistic is very large.  This value also aids in showing that the data is stationary.  Finally for the critical values as all being negative as well, this again shows that we can reject the null hypothesis and safely say that our GOOG data is stationary and move on with the next phase of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now since there are 200 stocks in our secondary stock data we will need to make a function to process the tests more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Secondary_Ticker  T-Statistic   P_value  \\\n",
      "0                RMD    -1.108040  0.880367   \n",
      "1               AMAT    -3.706228  0.017990   \n",
      "2               JNPR    -2.541904  0.260907   \n",
      "3               DECK    -2.104639  0.473942   \n",
      "4                JBL    -1.772805  0.642835   \n",
      "..               ...          ...       ...   \n",
      "195               ON    -1.469566  0.772676   \n",
      "196             SWKS    -0.594714  0.956864   \n",
      "197              MCD    -1.749258  0.654026   \n",
      "198             ADSK    -0.957321  0.910647   \n",
      "199                A    -1.563591  0.736010   \n",
      "\n",
      "                                       Critical_Values  Cointegrated  \n",
      "0    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "1    [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "2    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "3    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "4    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "..                                                 ...           ...  \n",
      "195  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "196  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "197  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "198  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "199  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "\n",
      "[200 rows x 5 columns]\n",
      "\n",
      "Cointegrated Pairs:\n",
      "    Secondary_Ticker  T-Statistic   P_value  \\\n",
      "1               AMAT    -3.706228  0.017990   \n",
      "45               STE    -3.637547  0.021999   \n",
      "78              NXPI    -4.405583  0.001743   \n",
      "108               BR    -3.384185  0.044138   \n",
      "119             ASML    -3.647295  0.021386   \n",
      "165            GOOGL    -4.030876  0.006486   \n",
      "\n",
      "                                       Critical_Values  Cointegrated  \n",
      "1    [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "45   [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "78   [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "108  [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "119  [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "165  [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n"
     ]
    }
   ],
   "source": [
    "# Let's make a function that runs the test for our core stock for each secondary stock we have generated.\n",
    "def cointegration_test(core_stock_data, sec_stock_data, core_ticker):\n",
    "    results = []\n",
    "    \n",
    "    core_data = core_stock_data[core_stock_data['Ticker'] == core_ticker]['Close_core']\n",
    "    \n",
    "    secondary_tickers = sec_stock_data['ticker'].unique()\n",
    "    \n",
    "    for ticker in secondary_tickers:\n",
    "        sec_data = sec_stock_data[sec_stock_data['ticker'] == ticker]['Close_sec']\n",
    "        \n",
    "        # As a check ensure the lengths match by trimming the larger series (if needed)\n",
    "        min_len = min(len(core_data), len(sec_data))\n",
    "        core_trimmed = core_data.iloc[-min_len:]\n",
    "        sec_trimmed = sec_data.iloc[-min_len:]\n",
    "        \n",
    "        # Perform the cointegration test.\n",
    "        coint_t, p_value, critical_values = coint(core_trimmed, sec_trimmed)\n",
    "        \n",
    "        # Append results to our results list\n",
    "        results.append({\n",
    "            'Secondary_Ticker' : ticker,\n",
    "            'T-Statistic' : coint_t,\n",
    "            'P_value' : p_value,\n",
    "            'Critical_Values' : critical_values,\n",
    "            'Cointegrated' : p_value < 0.05 # True if cointegrated\n",
    "        })\n",
    "\n",
    "    # Convert the results into a dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Call the function now and perform cointegration testing on subject core stock and all secondary stocks\n",
    "results_df = cointegration_test(core_stock_data, sec_stock_data, core_ticker='GOOG')\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Filter and display only the cointegrated pairs\n",
    "cointegrated_pairs = results_df[results_df['Cointegrated'] == True]\n",
    "print(\"\\nCointegrated Pairs:\")\n",
    "print(cointegrated_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neat!  Out of our 200 secondary stocks that we generated and preprocessed, we have 6 cointegration pairs with our core stock!  For this notebook using GOOG as our core stock it looks like AMAT, STE, NXPI, BR, ASML, and GOOGL (Applied Materials Inc, Steris PLC, NXP Semiconductors, Broadridge Financial Solutions, ASML Holding NV, and Google (yes there is more than one stock for Google)) are the cointegrated pairs as their p-value is less than 0.05 which is the deterministic stat for the cointegration test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now do the same cointegration test on our exogenous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Exogenous_Feature  T-Statistic   P_value  \\\n",
      "0                       nasdaq_100    -3.011563  0.107482   \n",
      "1                            sp500    -4.088657  0.005346   \n",
      "2              interest_rates_10yr    -1.726056  0.664895   \n",
      "3                             gold    -2.360375  0.343844   \n",
      "4                           copper    -2.242990  0.402318   \n",
      "5                         platinum    -1.211891  0.854760   \n",
      "6                           silver    -2.368816  0.339763   \n",
      "7                        crude_oil    -1.465590  0.774149   \n",
      "8                      natural_gas    -0.888169  0.922089   \n",
      "9                             corn    -0.767422  0.938852   \n",
      "10                           wheat    -0.769225  0.938630   \n",
      "11                volatility_index    -1.265170  0.840000   \n",
      "12           exchange_rate_usd_eur    -1.037919  0.895416   \n",
      "13           exchange_rate_usd_jpy    -1.537227  0.746642   \n",
      "14    dow_jones_industrial_average    -3.766288  0.015024   \n",
      "15       consumer_confidence_index    -1.200499  0.857770   \n",
      "16  vanguard_total_world_stock_etf    -3.383626  0.044202   \n",
      "17            us_treasury_bond_etf    -1.635187  0.705812   \n",
      "\n",
      "                                      Critical_Values  Cointegrated  \n",
      "0   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "1   [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "2   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "3   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "4   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "5   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "6   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "7   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "8   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "9   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "10  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "11  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "12  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "13  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "14  [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "15  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "16  [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "17  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "\n",
      "Cointegrated Exogenous Pairs:\n",
      "                 Exogenous_Feature  T-Statistic   P_value  \\\n",
      "1                            sp500    -4.088657  0.005346   \n",
      "14    dow_jones_industrial_average    -3.766288  0.015024   \n",
      "16  vanguard_total_world_stock_etf    -3.383626  0.044202   \n",
      "\n",
      "                                      Critical_Values  Cointegrated  \n",
      "1   [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "14  [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "16  [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n"
     ]
    }
   ],
   "source": [
    "# Let's make a function that runs the test for our core stock for each secondary stock we have generated.\n",
    "def cointegration_test(core_stock_data, exo_data_cleaned, core_ticker):\n",
    "    results = []\n",
    "    \n",
    "    core_data = core_stock_data[core_stock_data['Ticker'] == core_ticker]['Close_core']\n",
    "    \n",
    "    exo_features = ['nasdaq_100', 'sp500', 'interest_rates_10yr', 'gold', 'copper', 'platinum', 'silver', 'crude_oil', 'natural_gas', 'corn', 'wheat', 'volatility_index', 'exchange_rate_usd_eur', 'exchange_rate_usd_jpy', 'dow_jones_industrial_average', 'consumer_confidence_index', 'vanguard_total_world_stock_etf', 'us_treasury_bond_etf']\n",
    "    \n",
    "    for feature in exo_features:\n",
    "        exo_series = exo_data_cleaned[feature]\n",
    "        \n",
    "        # Perform the cointegration test.\n",
    "        coint_t, p_value, critical_values = coint(core_data, exo_series)\n",
    "        \n",
    "        # Append results to our results list\n",
    "        results.append({\n",
    "            'Exogenous_Feature' : feature,\n",
    "            'T-Statistic' : coint_t,\n",
    "            'P_value' : p_value,\n",
    "            'Critical_Values' : critical_values,\n",
    "            'Cointegrated' : p_value < 0.05 # True if cointegrated\n",
    "        })\n",
    "\n",
    "    # Convert the results into a dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Call the function now and perform cointegration testing on subject core stock and all secondary stocks\n",
    "results_df_exo = cointegration_test(core_stock_data, exo_data_cleaned, core_ticker='GOOG')\n",
    "\n",
    "print(results_df_exo)\n",
    "\n",
    "# Filter and display only the cointegrated pairs\n",
    "cointegrated_exo = results_df_exo[results_df_exo['Cointegrated'] == True]\n",
    "\n",
    "# In case no cointegration is found, we need some error logic.\n",
    "if cointegrated_exo.empty:\n",
    "    print(\"No exogenous features are cointegrated with our core ticker.\")\n",
    "else:\n",
    "    print(\"\\nCointegrated Exogenous Pairs:\")\n",
    "    print(cointegrated_exo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We actually have several of our exogenous features that cointegrated with our core ticker in this notebook this time, those being SP500, the Dow Jones Industrial Average, and the Vanguard Total World Stock ETF.  Let's plug those in along with our cointegrated features for more data.  GOOG did quite well in the cointegration test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok time for our Decision Tree models.  We have a lot of setup to do for them first too.  We need to rename our ticker feature columns in our core_stock_ and sec_stock_data dataframes so they don't overwrite each other when a merge is applied (will apply these here below).  Also since we talked about previously the dataframe indices matching now is the time to talk about our sec_stock_data.  We will just use our cointegrated_pairs for our Decision Tree modeling, though however many pairs we have is x that many indices of data worth of our target.  So rather than unnecessary align or prune it down, we will just do a separate tree for each cointegrated pair, and then look at bringing them together afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec_ticker</th>\n",
       "      <th>Close_sec</th>\n",
       "      <th>Volume_sec</th>\n",
       "      <th>Open_sec</th>\n",
       "      <th>High_sec</th>\n",
       "      <th>Low_sec</th>\n",
       "      <th>SMA_sec</th>\n",
       "      <th>EMA_sec</th>\n",
       "      <th>RSI_sec</th>\n",
       "      <th>BBM_sec</th>\n",
       "      <th>...</th>\n",
       "      <th>Momentum_7_Lag_Avg_1_3_sec</th>\n",
       "      <th>Momentum_7_Lag_Std_1_3_sec</th>\n",
       "      <th>Momentum_30_Lag_Avg_1_3_sec</th>\n",
       "      <th>Momentum_30_Lag_Std_1_3_sec</th>\n",
       "      <th>Momentum_50_Lag_Avg_1_3_sec</th>\n",
       "      <th>Momentum_50_Lag_Std_1_3_sec</th>\n",
       "      <th>OBV_Lag_Avg_1_3_sec</th>\n",
       "      <th>OBV_Lag_Std_1_3_sec</th>\n",
       "      <th>Diff_Close_EMA_sec</th>\n",
       "      <th>Ratio_Close_EMA_sec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>RMD</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>972500.0</td>\n",
       "      <td>102.570000</td>\n",
       "      <td>102.570000</td>\n",
       "      <td>100.959999</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.611779e+06</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>RMD</td>\n",
       "      <td>100.370003</td>\n",
       "      <td>2279400.0</td>\n",
       "      <td>100.900002</td>\n",
       "      <td>101.730003</td>\n",
       "      <td>100.199997</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>100.975294</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.611779e+06</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.994006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>RMD</td>\n",
       "      <td>97.400002</td>\n",
       "      <td>1915700.0</td>\n",
       "      <td>100.360001</td>\n",
       "      <td>100.610001</td>\n",
       "      <td>96.940002</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>100.835087</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.611779e+06</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.965934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>RMD</td>\n",
       "      <td>97.900002</td>\n",
       "      <td>1101100.0</td>\n",
       "      <td>97.660004</td>\n",
       "      <td>98.190002</td>\n",
       "      <td>97.070000</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>100.719985</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.100176e+06</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.972002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>RMD</td>\n",
       "      <td>98.809998</td>\n",
       "      <td>1467600.0</td>\n",
       "      <td>99.129997</td>\n",
       "      <td>100.800003</td>\n",
       "      <td>98.449997</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>100.645084</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>9.614140e+05</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.981767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sec_ticker   Close_sec  Volume_sec    Open_sec    High_sec  \\\n",
       "Date                                                                    \n",
       "2019-03-14        RMD  101.000000    972500.0  102.570000  102.570000   \n",
       "2019-03-15        RMD  100.370003   2279400.0  100.900002  101.730003   \n",
       "2019-03-18        RMD   97.400002   1915700.0  100.360001  100.610001   \n",
       "2019-03-19        RMD   97.900002   1101100.0   97.660004   98.190002   \n",
       "2019-03-20        RMD   98.809998   1467600.0   99.129997  100.800003   \n",
       "\n",
       "               Low_sec   SMA_sec     EMA_sec    RSI_sec   BBM_sec  ...  \\\n",
       "Date                                                               ...   \n",
       "2019-03-14  100.959999  104.9046  101.000000  63.818062  104.9046  ...   \n",
       "2019-03-15  100.199997  104.9046  100.975294  63.818062  104.9046  ...   \n",
       "2019-03-18   96.940002  104.9046  100.835087  63.818062  104.9046  ...   \n",
       "2019-03-19   97.070000  104.9046  100.719985  63.818062  104.9046  ...   \n",
       "2019-03-20   98.449997  104.9046  100.645084  63.818062  104.9046  ...   \n",
       "\n",
       "            Momentum_7_Lag_Avg_1_3_sec  Momentum_7_Lag_Std_1_3_sec  \\\n",
       "Date                                                                 \n",
       "2019-03-14                    0.419998                1.000000e-10   \n",
       "2019-03-15                    0.419998                1.000000e-10   \n",
       "2019-03-18                    0.419998                1.000000e-10   \n",
       "2019-03-19                    0.419998                1.000000e-10   \n",
       "2019-03-20                    0.419998                1.000000e-10   \n",
       "\n",
       "            Momentum_30_Lag_Avg_1_3_sec  Momentum_30_Lag_Std_1_3_sec  \\\n",
       "Date                                                                   \n",
       "2019-03-14                     2.940002                 1.000000e-10   \n",
       "2019-03-15                     2.940002                 1.000000e-10   \n",
       "2019-03-18                     2.940002                 1.000000e-10   \n",
       "2019-03-19                     2.940002                 1.000000e-10   \n",
       "2019-03-20                     2.940002                 1.000000e-10   \n",
       "\n",
       "            Momentum_50_Lag_Avg_1_3_sec  Momentum_50_Lag_Std_1_3_sec  \\\n",
       "Date                                                                   \n",
       "2019-03-14                    12.889999                 1.000000e-10   \n",
       "2019-03-15                    12.889999                 1.000000e-10   \n",
       "2019-03-18                    12.889999                 1.000000e-10   \n",
       "2019-03-19                    12.889999                 1.000000e-10   \n",
       "2019-03-20                    12.889999                 1.000000e-10   \n",
       "\n",
       "            OBV_Lag_Avg_1_3_sec  OBV_Lag_Std_1_3_sec  Diff_Close_EMA_sec  \\\n",
       "Date                                                                       \n",
       "2019-03-14         1.000000e-10         1.611779e+06        1.000000e-10   \n",
       "2019-03-15         1.000000e-10         1.611779e+06        1.000000e-10   \n",
       "2019-03-18         1.000000e-10         1.611779e+06        1.000000e-10   \n",
       "2019-03-19         1.000000e-10         2.100176e+06        1.000000e-10   \n",
       "2019-03-20         1.000000e-10         9.614140e+05        1.000000e-10   \n",
       "\n",
       "            Ratio_Close_EMA_sec  \n",
       "Date                             \n",
       "2019-03-14             1.000000  \n",
       "2019-03-15             0.994006  \n",
       "2019-03-18             0.965934  \n",
       "2019-03-19             0.972002  \n",
       "2019-03-20             0.981767  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's rename our two columns.\n",
    "core_stock_data.rename(columns = {'Ticker' : 'core_ticker'}, inplace = True)\n",
    "sec_stock_data.rename(columns = {'ticker' : 'sec_ticker'}, inplace = True)\n",
    "\n",
    "# Now let's make a variable that we can use for our cointegrated pairs\n",
    "cointegrated_stocks = ['AMAT', 'STE', 'NXPI', 'BR', 'ASML', 'GOOGL'] # To be edited for each core stock's results.\n",
    "\n",
    "sec_stock_data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The changes have been applied, and we are now ready to start preparing and building our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now set up our X,y for the model.  Similar to how we did for the Linear Regression models in this project we will eschew the traditional train_test_split methodology in favor of a more straightforward approach that works well with time series data.  We will also be using our cointegrated stocks here along with their data in the sec_stock_data dataframe and leaving the rest behind, and then adding the exo_data (exogenous data) to the mix to see what may be added here to the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Decision Tree for AMAT\n",
      "X_train shape: (1092, 10), X_test shape: (273, 10)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 18.83844972537079\n",
      "Test RMSE: 41.233394720481066\n",
      "Train MAPE: 15.720956398505889\n",
      "Test MAPE: 23.253393324383637\n",
      "Train R2: 0.5681224185292975\n",
      "Test R2: -3.884022894880104\n",
      "Train Adj R2: 0.5641272512631486\n",
      "Test Adj R2: -4.070435982470948\n",
      "Cross-Validation RMSE scores: [34.09597867 28.78770214 34.21553901 29.09194834 41.23339472]\n",
      "Mean Cross-Validation RMSE: 33.484912576064076\n",
      "Building Decision Tree for STE\n",
      "X_train shape: (1092, 10), X_test shape: (273, 10)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 25.540426314872587\n",
      "Test RMSE: 53.79658543856079\n",
      "Train MAPE: 23.66077298034085\n",
      "Test MAPE: 32.265164728755245\n",
      "Train R2: 0.2061716150397115\n",
      "Test R2: -7.3135962742143406\n",
      "Train Adj R2: 0.19882815171907975\n",
      "Test Adj R2: -7.630909109108018\n",
      "Cross-Validation RMSE scores: [56.39738939 37.38344087 31.97919437 18.32800575 53.79658544]\n",
      "Mean Cross-Validation RMSE: 39.57692316397699\n",
      "Building Decision Tree for NXPI\n",
      "X_train shape: (1092, 10), X_test shape: (273, 10)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 21.071570566760826\n",
      "Test RMSE: 47.85905508494399\n",
      "Train MAPE: 18.503521551470815\n",
      "Test MAPE: 27.81062250321976\n",
      "Train R2: 0.45966368895024556\n",
      "Test R2: -5.579725077316107\n",
      "Train Adj R2: 0.4546652031866031\n",
      "Test Adj R2: -5.830859622251837\n",
      "Cross-Validation RMSE scores: [46.40288209 36.55025299 34.68299552 28.30343145 47.85905508]\n",
      "Mean Cross-Validation RMSE: 38.75972342891578\n",
      "Building Decision Tree for BR\n",
      "X_train shape: (1092, 10), X_test shape: (273, 10)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 25.89102642094576\n",
      "Test RMSE: 55.0201488804747\n",
      "Train MAPE: 24.103171832372347\n",
      "Test MAPE: 32.78118584296003\n",
      "Train R2: 0.18422784848587626\n",
      "Test R2: -7.696070072606167\n",
      "Train Adj R2: 0.17668139009999173\n",
      "Test Adj R2: -8.027981144079686\n",
      "Cross-Validation RMSE scores: [56.83114691 36.05670982 34.55251131 17.24067666 55.02014888]\n",
      "Mean Cross-Validation RMSE: 39.94023871535162\n",
      "Building Decision Tree for ASML\n",
      "X_train shape: (1092, 10), X_test shape: (273, 10)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 17.956112347410162\n",
      "Test RMSE: 46.10017938781096\n",
      "Train MAPE: 14.835598721655693\n",
      "Test MAPE: 26.876100492743227\n",
      "Train R2: 0.6076307469622957\n",
      "Test R2: -5.1049869349455514\n",
      "Train Adj R2: 0.6040010591451106\n",
      "Test Adj R2: -5.338001703454924\n",
      "Cross-Validation RMSE scores: [29.94626596 35.76735145 27.54747413 31.05486391 46.10017939]\n",
      "Mean Cross-Validation RMSE: 34.0832269685775\n",
      "Building Decision Tree for GOOGL\n",
      "X_train shape: (1092, 10), X_test shape: (273, 10)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 22.758874447099288\n",
      "Test RMSE: 48.236705259092176\n",
      "Train MAPE: 20.181852452682758\n",
      "Test MAPE: 28.323174405800554\n",
      "Train R2: 0.3696643074106728\n",
      "Test R2: -5.683974443900573\n",
      "Train Adj R2: 0.3638332649260352\n",
      "Test Adj R2: -5.939087972293724\n",
      "Cross-Validation RMSE scores: [50.08518414 37.78019583 33.95869148 24.70017906 48.23670526]\n",
      "Mean Cross-Validation RMSE: 38.9521911541975\n",
      "Building Decision Tree for sp500\n",
      "X_train shape: (0, 10), X_test shape: (0, 10)\n",
      "y_train shape(0,), y_test shape: (1365,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(dec_tree, param_grid, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Fit the model.\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m best_tree \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     57\u001b[0m best_tree\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\Desktop\\Portfolio-Proj-WK\\DL-Strategies\\DL-Strat\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\Desktop\\Portfolio-Proj-WK\\DL-Strategies\\DL-Strat\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\Desktop\\Portfolio-Proj-WK\\DL-Strategies\\DL-Strat\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\Desktop\\Portfolio-Proj-WK\\DL-Strategies\\DL-Strat\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\Desktop\\Portfolio-Proj-WK\\DL-Strategies\\DL-Strat\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:409\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    407\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         (\n\u001b[0;32m    411\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    414\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=0."
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's set up our Decision Tree model and get it ready.\n",
    "# We have to do some prep work here.  We have our core data and cointegrated pairs that we have to make sure make it to X.\n",
    "\n",
    "# To hopefully improve our net metric scores and reduce noise in the model we will utilize our optimized features we acquired using our VIF and correlation matrix analysis in a previous notebook, that was also use to generate our sec_stock_data stocks.\n",
    "optimized_features = ['Volume_sec', 'Momentum_30_sec', 'MACD_Lag_Std_1_3_sec', 'MACD_Hist_Lag_Std_1_3_sec', 'MACD_Signal_Lag_Std_1_3_sec', 'Momentum_1_Lag_Std_1_3_sec', 'Momentum_3_Lag_Std_1_3_sec', 'Momentum_7_Lag_Std_1_3_sec', 'Momentum_30_Lag_Std_1_3_sec', 'Momentum_50_Lag_Std_1_3_sec']\n",
    "\n",
    "# Let's add a list as well for our exogenous features that cointegrated.  I want to have separate trees for this as well.\n",
    "cointegrated_exo_features = ['sp500', 'dow_jones_industrial_average', 'vanguard_total_world_stock_etf']\n",
    "\n",
    "# Let's first get the cointegrated stocks as a variable so we can use to grab their data from sec_stock_data.\n",
    "cointegrated_stocks = ['AMAT', 'STE', 'NXPI', 'BR', 'ASML', 'GOOGL', ] + cointegrated_exo_features\n",
    "\n",
    "# Loop over each cointegrated stock to build a separate tree\n",
    "for stock in cointegrated_stocks:\n",
    "    print(f\"Building Decision Tree for {stock}\")\n",
    "\n",
    "    # Let's filter our sec_stock_data for rows where the ticker is included in our cointegrated_tickers so we can grab that data together.\n",
    "    sec_stock_filtered = sec_stock_data[sec_stock_data['sec_ticker'] == stock]\n",
    "    \n",
    "    # Setting up X with the cointegrated stock data that has the excluded features.\n",
    "    X = sec_stock_filtered[optimized_features]\n",
    "    \n",
    "    # Ensure all datatypes for our X are numeric\n",
    "    X = X.apply(pd.to_numeric, errors = 'coerce')\n",
    "    y = goog_data_orig['Close_core'] # Our target\n",
    "    \n",
    "    \n",
    "\n",
    "    # Not done yet, we are going to use split_index for Time Series on our X and y.\n",
    "    split_index = int(len(X) * 0.8)\n",
    "\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "    \n",
    "    # Let's check shapes of X and y to make sure\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape{y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    # Set up the Decision Tree model itself.\n",
    "    dec_tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    # Set up the parameter grid for tuning if needed.\n",
    "    param_grid = {\n",
    "        'max_depth' : [2, 3, 5],\n",
    "        'min_samples_split' : [10, 20],\n",
    "        'min_samples_leaf' : [5, 10]\n",
    "    }\n",
    "\n",
    "    # Use GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(dec_tree, param_grid, cv = 3, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "    # Fit the model.\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_tree = grid_search.best_estimator_\n",
    "\n",
    "    best_tree.fit(X_train, y_train)\n",
    "\n",
    "    # Now we can make predictions\n",
    "    y_train_pred = best_tree.predict(X_train)\n",
    "    y_test_pred = best_tree.predict(X_test)\n",
    "    \n",
    "    # Let's calculate the RMSE first.\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    # Let's now calculate the MAPE.\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "    mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "    # Now for the R2 scores.\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Lastly the Adjusted R2 score.\n",
    "    def adjusted_r2(r2, X):\n",
    "        n = len(X)\n",
    "        p = X.shape[1]\n",
    "        return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "    adj_r2_train = adjusted_r2(r2_train, X_train)\n",
    "    adj_r2_test = adjusted_r2(r2_test, X_test)\n",
    "\n",
    "    print(f\"Train RMSE:\", rmse_train)\n",
    "    print(f\"Test RMSE:\", rmse_test)\n",
    "\n",
    "    print(f\"Train MAPE:\", mape_train)\n",
    "    print(f\"Test MAPE:\", mape_test)\n",
    "\n",
    "    print(f\"Train R2:\", r2_train)\n",
    "    print(f\"Test R2:\", r2_test)\n",
    "\n",
    "    print(f\"Train Adj R2:\", adj_r2_train)\n",
    "    print(f\"Test Adj R2:\", adj_r2_test)\n",
    "    \n",
    "    # Now finally adding cross_validation to evaluate model stability\n",
    "    cv_scores = cross_val_score(best_tree, X, y, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores)\n",
    "    \n",
    "    print(f\"Cross-Validation RMSE scores: {cv_rmse}\")\n",
    "    print(f\"Mean Cross-Validation RMSE: {cv_rmse.mean()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot metrics obs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at a few plots based on our Decision Tree model.  First let's just take a look at the tree itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "\n",
    "plot_tree(best_tree, feature_names = X.columns, filled = True, rounded = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at another plot regarding feature importance to see how the model is using the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first make a quick variable for our feature importances to use in the plot.\n",
    "feature_importances = best_tree.feature_importances_\n",
    "\n",
    "# make these into a dataframe so they are visualized better\n",
    "feature_df = pd.DataFrame({'Feature' : X.columns, 'Importance' : feature_importances})\n",
    "\n",
    "# Sort these features by their importance.\n",
    "feature_df = feature_df.sort_values(by = 'Importance', ascending=False)\n",
    "\n",
    "# Now we can plot and see how it looks.\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = feature_df)\n",
    "plt.title('Feature Importance in Decision Tree Model of AAPL Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot observation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One more, let's look at an Actual vs Predicted for our values for the AAPL data from our Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(y_test.values, label = 'Actual Values', color = 'blue')\n",
    "plt.plot(y_test_pred, 'Predicted Values', color = 'red')\n",
    "plt.title('Actual vs Predicted Values for AAPL Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Observation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Strat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
