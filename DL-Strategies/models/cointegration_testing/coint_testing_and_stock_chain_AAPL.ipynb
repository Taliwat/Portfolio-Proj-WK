{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cointegration Testing Analysis and Decision Tree Modeling for the AAPL Stock**\n",
    "## In this notebook we start to bring things together from other sections of the project.  We combine our analyzed and preprocessed core_stock_data with our preprocessed and dynamically generated secondary stock data to perform cointegration testing to see if we can create cointegrated pairs, which we will add as separate features to use for our Decision Tree model.  We will also merge in our exogenous data for this Decision Tree model, data we haven't talked about a whole lot yet.  This data will aid in adding in another layer to the stock chains we want to create at the end output at the end of the Decision Tree based on their relationships.  We will use the output of the Decision Tree to read into how the stocks work with each other so we can start to formulate our trading strategy in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As with our other notebooks let's read in our data to use for this.  This time let's take a good look at the shape of our dataframes since we will plan on merging them later on in the notebook.  It will be important that the dataframes match in size so that modeling works for our Decision Trees.  If they mismatch we will have to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn import tree\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Close_core  Volume_core  Open_core  High_core   Low_core  \\\n",
      "Date                                                                   \n",
      "2019-03-14   45.932499   94318000.0  45.974998  46.025002  45.639999   \n",
      "2019-03-15   46.529999  156171600.0  46.212502  46.832500  45.935001   \n",
      "2019-03-18   47.005001  104879200.0  46.450001  47.097500  46.447498   \n",
      "2019-03-19   46.632500  126585600.0  47.087502  47.247501  46.480000   \n",
      "2019-03-20   47.040001  124140800.0  46.557499  47.372501  46.182499   \n",
      "\n",
      "            SMA_core   EMA_core   RSI_core  BBM_core   BBU_core  ...  \\\n",
      "Date                                                             ...   \n",
      "2019-03-14  41.35925  42.219051  75.741602  41.35925  46.695085  ...   \n",
      "2019-03-15  41.50025  42.388107  76.985910  41.50025  47.003365  ...   \n",
      "2019-03-18  41.72940  42.569162  78.724282  41.72940  47.174667  ...   \n",
      "2019-03-19  41.92075  42.728509  73.527018  41.92075  47.369412  ...   \n",
      "2019-03-20  42.12190  42.897587  80.396901  42.12190  47.569044  ...   \n",
      "\n",
      "            Momentum_7_Lag_Avg_1_3_core  Momentum_7_Lag_Std_1_3_core  \\\n",
      "Date                                                                   \n",
      "2019-03-14                     2.049999                     0.601040   \n",
      "2019-03-15                     2.049999                     0.601040   \n",
      "2019-03-18                     2.474998                     0.601040   \n",
      "2019-03-19                     2.943333                     0.915770   \n",
      "2019-03-20                     3.394999                     0.490078   \n",
      "\n",
      "            Momentum_30_Lag_Avg_1_3_core  Momentum_30_Lag_Std_1_3_core  \\\n",
      "Date                                                                     \n",
      "2019-03-14                      4.619999                      0.212131   \n",
      "2019-03-15                      4.619999                      0.212131   \n",
      "2019-03-18                      4.769999                      0.212131   \n",
      "2019-03-19                      4.971666                      0.380143   \n",
      "2019-03-20                      4.704999                      0.799484   \n",
      "\n",
      "            Momentum_50_Lag_Avg_1_3_core  Momentum_50_Lag_Std_1_3_core  \\\n",
      "Date                                                                     \n",
      "2019-03-14                      7.049999                  1.000000e-10   \n",
      "2019-03-15                      7.049999                  1.000000e-10   \n",
      "2019-03-18                      7.049999                  1.000000e-10   \n",
      "2019-03-19                      8.519166                  2.544672e+00   \n",
      "2019-03-20                      9.358334                  2.211183e+00   \n",
      "\n",
      "            OBV_Lag_Avg_1_3_core  OBV_Lag_Std_1_3_core  Diff_Close_EMA_core  \\\n",
      "Date                                                                          \n",
      "2019-03-14          1.592191e+09          1.104300e+08             3.713448   \n",
      "2019-03-15          1.592191e+09          1.104300e+08             4.141891   \n",
      "2019-03-18          1.670277e+09          1.104300e+08             4.435839   \n",
      "2019-03-19          1.731265e+09          1.313626e+08             3.903991   \n",
      "2019-03-20          1.776087e+09          6.769383e+07             4.142414   \n",
      "\n",
      "            Ratio_Close_EMA_core  \n",
      "Date                              \n",
      "2019-03-14              1.087957  \n",
      "2019-03-15              1.097714  \n",
      "2019-03-18              1.104203  \n",
      "2019-03-19              1.091367  \n",
      "2019-03-20              1.096565  \n",
      "\n",
      "[5 rows x 153 columns]\n",
      "(13650, 153)\n",
      "           ticker   Close_sec  Volume_sec    Open_sec    High_sec     Low_sec  \\\n",
      "Date                                                                            \n",
      "2019-03-14    RMD  101.000000    972500.0  102.570000  102.570000  100.959999   \n",
      "2019-03-15    RMD  100.370003   2279400.0  100.900002  101.730003  100.199997   \n",
      "2019-03-18    RMD   97.400002   1915700.0  100.360001  100.610001   96.940002   \n",
      "2019-03-19    RMD   97.900002   1101100.0   97.660004   98.190002   97.070000   \n",
      "2019-03-20    RMD   98.809998   1467600.0   99.129997  100.800003   98.449997   \n",
      "\n",
      "             SMA_sec     EMA_sec    RSI_sec   BBM_sec  ...  \\\n",
      "Date                                                   ...   \n",
      "2019-03-14  104.9046  101.000000  63.818062  104.9046  ...   \n",
      "2019-03-15  104.9046  100.975294  63.818062  104.9046  ...   \n",
      "2019-03-18  104.9046  100.835087  63.818062  104.9046  ...   \n",
      "2019-03-19  104.9046  100.719985  63.818062  104.9046  ...   \n",
      "2019-03-20  104.9046  100.645084  63.818062  104.9046  ...   \n",
      "\n",
      "            Momentum_7_Lag_Avg_1_3_sec  Momentum_7_Lag_Std_1_3_sec  \\\n",
      "Date                                                                 \n",
      "2019-03-14                    0.419998                1.000000e-10   \n",
      "2019-03-15                    0.419998                1.000000e-10   \n",
      "2019-03-18                    0.419998                1.000000e-10   \n",
      "2019-03-19                    0.419998                1.000000e-10   \n",
      "2019-03-20                    0.419998                1.000000e-10   \n",
      "\n",
      "            Momentum_30_Lag_Avg_1_3_sec  Momentum_30_Lag_Std_1_3_sec  \\\n",
      "Date                                                                   \n",
      "2019-03-14                     2.940002                 1.000000e-10   \n",
      "2019-03-15                     2.940002                 1.000000e-10   \n",
      "2019-03-18                     2.940002                 1.000000e-10   \n",
      "2019-03-19                     2.940002                 1.000000e-10   \n",
      "2019-03-20                     2.940002                 1.000000e-10   \n",
      "\n",
      "            Momentum_50_Lag_Avg_1_3_sec  Momentum_50_Lag_Std_1_3_sec  \\\n",
      "Date                                                                   \n",
      "2019-03-14                    12.889999                 1.000000e-10   \n",
      "2019-03-15                    12.889999                 1.000000e-10   \n",
      "2019-03-18                    12.889999                 1.000000e-10   \n",
      "2019-03-19                    12.889999                 1.000000e-10   \n",
      "2019-03-20                    12.889999                 1.000000e-10   \n",
      "\n",
      "            OBV_Lag_Avg_1_3_sec  OBV_Lag_Std_1_3_sec  Diff_Close_EMA_sec  \\\n",
      "Date                                                                       \n",
      "2019-03-14         1.000000e-10         1.611779e+06        1.000000e-10   \n",
      "2019-03-15         1.000000e-10         1.611779e+06        1.000000e-10   \n",
      "2019-03-18         1.000000e-10         1.611779e+06        1.000000e-10   \n",
      "2019-03-19         1.000000e-10         2.100176e+06        1.000000e-10   \n",
      "2019-03-20         1.000000e-10         9.614140e+05        1.000000e-10   \n",
      "\n",
      "            Ratio_Close_EMA_sec  \n",
      "Date                             \n",
      "2019-03-14             1.000000  \n",
      "2019-03-15             0.994006  \n",
      "2019-03-18             0.965934  \n",
      "2019-03-19             0.972002  \n",
      "2019-03-20             0.981767  \n",
      "\n",
      "[5 rows x 153 columns]\n",
      "(266665, 153)\n",
      "            interest_rates_10yr         gold  copper    platinum  silver  \\\n",
      "Date                                                                       \n",
      "2019-03-14                2.630  1293.400024  2.8885  825.599976  15.101   \n",
      "2019-03-15                2.593  1301.800049  2.9035  830.299988  15.253   \n",
      "2019-03-18                2.602  1300.300049  2.9035  832.599976  15.251   \n",
      "2019-03-19                2.614  1305.000000  2.9195  851.200012  15.301   \n",
      "2019-03-20                2.535  1300.500000  2.9155  858.200012  15.245   \n",
      "\n",
      "            crude_oil  natural_gas    corn   wheat  volatility_index  ...  \\\n",
      "Date                                                                  ...   \n",
      "2019-03-14  58.610001        2.855  361.50  448.25             13.50  ...   \n",
      "2019-03-15  58.520000        2.795  373.25  462.25             12.88  ...   \n",
      "2019-03-18  59.090000        2.850  371.50  456.75             13.10  ...   \n",
      "2019-03-19  59.029999        2.874  371.25  456.50             13.56  ...   \n",
      "2019-03-20  59.830002        2.820  371.50  464.75             13.91  ...   \n",
      "\n",
      "            vanguard_total_world_stock_etf_Diff  us_treasury_bond_etf_Lag_1  \\\n",
      "Date                                                                          \n",
      "2019-03-14                         5.699997e-01                   24.870001   \n",
      "2019-03-15                         5.699997e-01                   24.870001   \n",
      "2019-03-18                         3.300018e-01                   24.920000   \n",
      "2019-03-19                         1.000214e-02                   24.910000   \n",
      "2019-03-20                         1.000000e-10                   24.900000   \n",
      "\n",
      "            us_treasury_bond_etf_Lag_2  us_treasury_bond_etf_Lag_3  \\\n",
      "Date                                                                 \n",
      "2019-03-14                   24.870001                   24.870001   \n",
      "2019-03-15                   24.870001                   24.870001   \n",
      "2019-03-18                   24.870001                   24.870001   \n",
      "2019-03-19                   24.920000                   24.870001   \n",
      "2019-03-20                   24.910000                   24.920000   \n",
      "\n",
      "            us_treasury_bond_etf_MA_7  us_treasury_bond_etf_MA_30  \\\n",
      "Date                                                                \n",
      "2019-03-14                  24.967143                      25.044   \n",
      "2019-03-15                  24.967143                      25.044   \n",
      "2019-03-18                  24.967143                      25.044   \n",
      "2019-03-19                  24.967143                      25.044   \n",
      "2019-03-20                  24.967143                      25.044   \n",
      "\n",
      "            us_treasury_bond_etf_Std_7  us_treasury_bond_etf_Std_30  \\\n",
      "Date                                                                  \n",
      "2019-03-14                    0.097076                     0.102876   \n",
      "2019-03-15                    0.097076                     0.102876   \n",
      "2019-03-18                    0.097076                     0.102876   \n",
      "2019-03-19                    0.097076                     0.102876   \n",
      "2019-03-20                    0.097076                     0.102876   \n",
      "\n",
      "            us_treasury_bond_etf_RoC  us_treasury_bond_etf_Diff  \n",
      "Date                                                             \n",
      "2019-03-14              2.010424e-01               4.999924e-02  \n",
      "2019-03-15              2.010424e-01               4.999924e-02  \n",
      "2019-03-18              1.000000e-10               1.000000e-10  \n",
      "2019-03-19              1.000000e-10               1.000000e-10  \n",
      "2019-03-20              4.417695e-01               1.100006e-01  \n",
      "\n",
      "[5 rows x 180 columns]\n",
      "(1415, 180)\n"
     ]
    }
   ],
   "source": [
    "# Now let's access the main core_stock_data.csv file, as well as the secondary stock data and exo_data to all be used in this notebook!\n",
    "csv_path = os.path.join(project_root, 'data', 'core_stock_unscaled.csv')\n",
    "core_stock_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "print(core_stock_data.head())\n",
    "print(core_stock_data.shape)\n",
    "\n",
    "csv_path = os.path.join(project_root, 'data', 'sec_stock_unscaled.csv')\n",
    "sec_stock_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "print(sec_stock_data.head())\n",
    "print(sec_stock_data.shape)\n",
    "\n",
    "csv_path = os.path.join(project_root, 'data', 'exo_data_unscaled.csv')\n",
    "exo_data = pd.read_csv(csv_path, parse_dates=['Date'], index_col= 'Date')\n",
    "\n",
    "print(exo_data.head())\n",
    "print(exo_data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-04-19', '2019-05-27', '2019-07-04', '2019-09-02',\n",
      "               '2019-11-28', '2019-12-25', '2020-01-01', '2020-01-20',\n",
      "               '2020-02-17', '2020-04-10', '2020-05-25', '2020-07-03',\n",
      "               '2020-09-07', '2020-11-26', '2020-12-25', '2021-01-01',\n",
      "               '2021-01-18', '2021-02-15', '2021-04-02', '2021-05-31',\n",
      "               '2021-07-05', '2021-09-06', '2021-11-25', '2021-12-24',\n",
      "               '2022-01-17', '2022-02-21', '2022-04-15', '2022-05-30',\n",
      "               '2022-06-20', '2022-07-04', '2022-09-05', '2022-11-24',\n",
      "               '2022-12-26', '2023-01-02', '2023-01-16', '2023-02-20',\n",
      "               '2023-04-07', '2023-05-29', '2023-06-19', '2023-07-04',\n",
      "               '2023-09-04', '2023-11-23', '2023-12-25', '2024-01-01',\n",
      "               '2024-01-15', '2024-02-19', '2024-03-29', '2024-05-27',\n",
      "               '2024-06-19', '2024-07-04'],\n",
      "              dtype='datetime64[ns]', name='Date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "exo_only_dates = exo_data.index.difference(core_stock_data.index)\n",
    "print(exo_only_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's grab just our AAPL stock for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1365, 153)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will want to preserve this version of our data since we will be doing merges later on, once we merge it will exponentially increase how much data is contained in our subject ticker variable.\n",
    "aapl_data_orig = core_stock_data[core_stock_data['Ticker'] == 'AAPL'].copy()\n",
    "\n",
    "\n",
    "aapl_data_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now looking back at exo_data I see a mismatch in our data.  Sec_stock_data is a different story as we still need it as it for now, and we will look at it later.  For now though let's look into exo_data and fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-04-19', '2019-05-27', '2019-07-04', '2019-09-02',\n",
      "               '2019-11-28', '2019-12-25', '2020-01-01', '2020-01-20',\n",
      "               '2020-02-17', '2020-04-10', '2020-05-25', '2020-07-03',\n",
      "               '2020-09-07', '2020-11-26', '2020-12-25', '2021-01-01',\n",
      "               '2021-01-18', '2021-02-15', '2021-04-02', '2021-05-31',\n",
      "               '2021-07-05', '2021-09-06', '2021-11-25', '2021-12-24',\n",
      "               '2022-01-17', '2022-02-21', '2022-04-15', '2022-05-30',\n",
      "               '2022-06-20', '2022-07-04', '2022-09-05', '2022-11-24',\n",
      "               '2022-12-26', '2023-01-02', '2023-01-16', '2023-02-20',\n",
      "               '2023-04-07', '2023-05-29', '2023-06-19', '2023-07-04',\n",
      "               '2023-09-04', '2023-11-23', '2023-12-25', '2024-01-01',\n",
      "               '2024-01-15', '2024-02-19', '2024-03-29', '2024-05-27',\n",
      "               '2024-06-19', '2024-07-04'],\n",
      "              dtype='datetime64[ns]', name='Date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "exo_only_dates = exo_data.index.difference(aapl_data_orig.index)\n",
    "print(exo_only_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 50 different dates and values that our exo_data has that our core target data does not.  Upon first glance they appear to be related to holidays.  Let's remove these values from exo_data so that the indices align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exo_data shape after dropping extra values: (1365, 180)\n"
     ]
    }
   ],
   "source": [
    "exo_data_cleaned = exo_data.drop(index = exo_only_dates)\n",
    "\n",
    "# Now re-check shape\n",
    "print(f\"Exo_data shape after dropping extra values: {exo_data_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready.  We have our core stock and our secondary stock data.  Before we do the cointegration tests we will perform a ADF (Augmented Dickey-Fuller) test to check for stationarity.  This will help our cointegration results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Statistic: -0.9854132994930892\n",
      "p-value: 0.7585472175807795\n",
      "Critical Values: {'1%': -3.435153261837347, '5%': -2.8636612797849814, '10%': -2.567899369049974}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform ADF Test on the 'Close' price column\n",
    "adf_result = adfuller(aapl_data_orig['Close_core'])\n",
    "\n",
    "# Display ADF test results\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "print('Critical Values:', adf_result[4])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's break down our results.  The ADF Statistic as a negative number is expected and a higher negative number here suggests stronger evidence against our hypothesis that the data is non-stationary.  The output here is fairly large which means the data is very likely to be stationary.  For the p value a value of 0 can occur when the ADF statistic is very large.  This value also aids in showing that the data is stationary.  Finally for the critical values as all being negative as well, this again shows that we can reject the null hypothesis and safely say that our AAPL data is stationary and move on with the next phase of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now since there are 200 stocks in our secondary stock data we will need to make a function to process the tests more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Secondary_Ticker  T-Statistic   P_value  \\\n",
      "0                RMD    -0.814387  0.932787   \n",
      "1               AMAT    -2.290508  0.378299   \n",
      "2               JNPR    -2.632174  0.224915   \n",
      "3               DECK    -2.150604  0.449963   \n",
      "4                JBL    -1.825677  0.617168   \n",
      "..               ...          ...       ...   \n",
      "195               ON    -1.828137  0.615957   \n",
      "196             SWKS    -0.497450  0.964513   \n",
      "197              MCD    -2.279811  0.383669   \n",
      "198             ADSK    -0.781050  0.937149   \n",
      "199                A    -1.380142  0.804239   \n",
      "\n",
      "                                       Critical_Values  Cointegrated  \n",
      "0    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "1    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "2    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "3    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "4    [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "..                                                 ...           ...  \n",
      "195  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "196  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "197  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "198  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "199  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "\n",
      "[200 rows x 5 columns]\n",
      "\n",
      "Cointegrated Pairs:\n",
      "    Secondary_Ticker  T-Statistic   P_value  \\\n",
      "15               AZN    -3.350304  0.048178   \n",
      "85               ROL    -3.338208  0.049692   \n",
      "173             VTRS    -3.453472  0.036749   \n",
      "\n",
      "                                       Critical_Values  Cointegrated  \n",
      "15   [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "85   [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n",
      "173  [-3.9044872726627737, -3.340613212756168, -3.0...          True  \n"
     ]
    }
   ],
   "source": [
    "# Let's make a function that runs the test for our core stock for each secondary stock we have generated.\n",
    "def cointegration_test(core_stock_data, sec_stock_data, core_ticker):\n",
    "    results = []\n",
    "    \n",
    "    core_data = core_stock_data[core_stock_data['Ticker'] == core_ticker]['Close_core']\n",
    "    \n",
    "    secondary_tickers = sec_stock_data['ticker'].unique()\n",
    "    \n",
    "    for ticker in secondary_tickers:\n",
    "        sec_data = sec_stock_data[sec_stock_data['ticker'] == ticker]['Close_sec']\n",
    "        \n",
    "        # As a check ensure the lengths match by trimming the larger series (if needed)\n",
    "        min_len = min(len(core_data), len(sec_data))\n",
    "        core_trimmed = core_data.iloc[-min_len:]\n",
    "        sec_trimmed = sec_data.iloc[-min_len:]\n",
    "        \n",
    "        # Perform the cointegration test.\n",
    "        coint_t, p_value, critical_values = coint(core_trimmed, sec_trimmed)\n",
    "        \n",
    "        # Append results to our results list\n",
    "        results.append({\n",
    "            'Secondary_Ticker' : ticker,\n",
    "            'T-Statistic' : coint_t,\n",
    "            'P_value' : p_value,\n",
    "            'Critical_Values' : critical_values,\n",
    "            'Cointegrated' : p_value < 0.05 # True if cointegrated\n",
    "        })\n",
    "\n",
    "    # Convert the results into a dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Call the function now and perform cointegration testing on subject core stock and all secondary stocks\n",
    "results_df = cointegration_test(core_stock_data, sec_stock_data, core_ticker='AAPL')\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Filter and display only the cointegrated pairs\n",
    "cointegrated_pairs = results_df[results_df['Cointegrated'] == True]\n",
    "print(\"\\nCointegrated Pairs:\")\n",
    "print(cointegrated_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neat!  Out of our 200 secondary stocks that we generated and preprocessed, we have 3 cointegration pairs with our core stock!  For this notebook using AAPL as our core stock it looks like AZN, ROL, and VTRS (AstraZeneca PLC, Rollins Inc, and Viatris Inc respectively) are the cointegrated pairs as their p-value is less than 0.05 which is the deterministic stat for the cointegration test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now do the same cointegration test on our exogenous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Exogenous_Feature  T-Statistic   P_value  \\\n",
      "0                       nasdaq_100    -1.595764  0.722675   \n",
      "1                            sp500    -2.174622  0.437486   \n",
      "2              interest_rates_10yr    -2.100665  0.476020   \n",
      "3                             gold    -2.543518  0.260221   \n",
      "4                           copper    -1.800987  0.629243   \n",
      "5                         platinum    -1.119751  0.877682   \n",
      "6                           silver    -2.273159  0.387020   \n",
      "7                        crude_oil    -1.722480  0.666556   \n",
      "8                      natural_gas    -0.837346  0.929616   \n",
      "9                             corn    -0.550604  0.960528   \n",
      "10                           wheat    -0.961956  0.909828   \n",
      "11                volatility_index    -1.264144  0.840295   \n",
      "12           exchange_rate_usd_eur    -1.147543  0.871107   \n",
      "13           exchange_rate_usd_jpy    -2.073559  0.490193   \n",
      "14    dow_jones_industrial_average    -2.709884  0.196044   \n",
      "15       consumer_confidence_index    -1.449701  0.779970   \n",
      "16  vanguard_total_world_stock_etf    -1.939308  0.559946   \n",
      "17            us_treasury_bond_etf    -2.007994  0.524420   \n",
      "\n",
      "                                      Critical_Values  Cointegrated  \n",
      "0   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "1   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "2   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "3   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "4   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "5   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "6   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "7   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "8   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "9   [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "10  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "11  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "12  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "13  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "14  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "15  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "16  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "17  [-3.9044872726627737, -3.340613212756168, -3.0...         False  \n",
      "No exogenous features are cointegrated with our core ticker.\n"
     ]
    }
   ],
   "source": [
    "# Let's make a function that runs the test for our core stock for each exogenous feature we have generated.\n",
    "def cointegration_test(core_stock_data, exo_data_cleaned, core_ticker):\n",
    "    results = []\n",
    "    \n",
    "    core_data = core_stock_data[core_stock_data['Ticker'] == core_ticker]['Close_core']\n",
    "    \n",
    "    exo_features = ['nasdaq_100', 'sp500', 'interest_rates_10yr', 'gold', 'copper', 'platinum', 'silver', 'crude_oil', 'natural_gas', 'corn', 'wheat', 'volatility_index', 'exchange_rate_usd_eur', 'exchange_rate_usd_jpy', 'dow_jones_industrial_average', 'consumer_confidence_index', 'vanguard_total_world_stock_etf', 'us_treasury_bond_etf']\n",
    "    \n",
    "    for feature in exo_features:\n",
    "        exo_series = exo_data_cleaned[feature]\n",
    "        \n",
    "        # Perform the cointegration test.\n",
    "        coint_t, p_value, critical_values = coint(core_data, exo_series)\n",
    "        \n",
    "        # Append results to our results list\n",
    "        results.append({\n",
    "            'Exogenous_Feature' : feature,\n",
    "            'T-Statistic' : coint_t,\n",
    "            'P_value' : p_value,\n",
    "            'Critical_Values' : critical_values,\n",
    "            'Cointegrated' : p_value < 0.05 # True if cointegrated\n",
    "        })\n",
    "\n",
    "    # Convert the results into a dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Call the function now and perform cointegration testing on subject core stock and all secondary stocks\n",
    "results_df_exo = cointegration_test(core_stock_data, exo_data_cleaned, core_ticker='AAPL')\n",
    "\n",
    "print(results_df_exo)\n",
    "\n",
    "# Filter and display only the cointegrated pairs\n",
    "cointegrated_exo = results_df_exo[results_df_exo['Cointegrated'] == True]\n",
    "\n",
    "# In case no cointegration is found, we need some error logic.\n",
    "if cointegrated_exo.empty:\n",
    "    print(\"No exogenous features are cointegrated with our core ticker.\")\n",
    "else:\n",
    "    print(\"\\nCointegrated Exogenous Pairs:\")\n",
    "    print(cointegrated_exo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfortunately none of our exogenous features are cointegrated with our core ticker in this notebook at this time.  At another time we can look at expanding on our list of features in this space or the time period used but right now we will just move forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok time for our Decision Tree models.  We have a lot of setup to do for them first too.  We need to rename our ticker feature columns in our core_stock_ and sec_stock_data dataframes so they don't overwrite each other when a merge is applied (will apply these here below).  Also since we talked about previously the dataframe indices matching now is the time to talk about our sec_stock_data.  We will just use our cointegrated_pairs for our Decision Tree modeling, though however many pairs we have is x that many indices of data worth of our target.  So rather than unnecessary align or prune it down, we will just do a separate tree for each cointegrated pair, and then look at bringing them together afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec_ticker</th>\n",
       "      <th>Close_sec</th>\n",
       "      <th>Volume_sec</th>\n",
       "      <th>Open_sec</th>\n",
       "      <th>High_sec</th>\n",
       "      <th>Low_sec</th>\n",
       "      <th>SMA_sec</th>\n",
       "      <th>EMA_sec</th>\n",
       "      <th>RSI_sec</th>\n",
       "      <th>BBM_sec</th>\n",
       "      <th>...</th>\n",
       "      <th>Momentum_7_Lag_Avg_1_3_sec</th>\n",
       "      <th>Momentum_7_Lag_Std_1_3_sec</th>\n",
       "      <th>Momentum_30_Lag_Avg_1_3_sec</th>\n",
       "      <th>Momentum_30_Lag_Std_1_3_sec</th>\n",
       "      <th>Momentum_50_Lag_Avg_1_3_sec</th>\n",
       "      <th>Momentum_50_Lag_Std_1_3_sec</th>\n",
       "      <th>OBV_Lag_Avg_1_3_sec</th>\n",
       "      <th>OBV_Lag_Std_1_3_sec</th>\n",
       "      <th>Diff_Close_EMA_sec</th>\n",
       "      <th>Ratio_Close_EMA_sec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>RMD</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>972500.0</td>\n",
       "      <td>102.570000</td>\n",
       "      <td>102.570000</td>\n",
       "      <td>100.959999</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.611779e+06</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>RMD</td>\n",
       "      <td>100.370003</td>\n",
       "      <td>2279400.0</td>\n",
       "      <td>100.900002</td>\n",
       "      <td>101.730003</td>\n",
       "      <td>100.199997</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>100.975294</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.611779e+06</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.994006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>RMD</td>\n",
       "      <td>97.400002</td>\n",
       "      <td>1915700.0</td>\n",
       "      <td>100.360001</td>\n",
       "      <td>100.610001</td>\n",
       "      <td>96.940002</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>100.835087</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.611779e+06</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.965934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>RMD</td>\n",
       "      <td>97.900002</td>\n",
       "      <td>1101100.0</td>\n",
       "      <td>97.660004</td>\n",
       "      <td>98.190002</td>\n",
       "      <td>97.070000</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>100.719985</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.100176e+06</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.972002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>RMD</td>\n",
       "      <td>98.809998</td>\n",
       "      <td>1467600.0</td>\n",
       "      <td>99.129997</td>\n",
       "      <td>100.800003</td>\n",
       "      <td>98.449997</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>100.645084</td>\n",
       "      <td>63.818062</td>\n",
       "      <td>104.9046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.940002</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>12.889999</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>9.614140e+05</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.981767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sec_ticker   Close_sec  Volume_sec    Open_sec    High_sec  \\\n",
       "Date                                                                    \n",
       "2019-03-14        RMD  101.000000    972500.0  102.570000  102.570000   \n",
       "2019-03-15        RMD  100.370003   2279400.0  100.900002  101.730003   \n",
       "2019-03-18        RMD   97.400002   1915700.0  100.360001  100.610001   \n",
       "2019-03-19        RMD   97.900002   1101100.0   97.660004   98.190002   \n",
       "2019-03-20        RMD   98.809998   1467600.0   99.129997  100.800003   \n",
       "\n",
       "               Low_sec   SMA_sec     EMA_sec    RSI_sec   BBM_sec  ...  \\\n",
       "Date                                                               ...   \n",
       "2019-03-14  100.959999  104.9046  101.000000  63.818062  104.9046  ...   \n",
       "2019-03-15  100.199997  104.9046  100.975294  63.818062  104.9046  ...   \n",
       "2019-03-18   96.940002  104.9046  100.835087  63.818062  104.9046  ...   \n",
       "2019-03-19   97.070000  104.9046  100.719985  63.818062  104.9046  ...   \n",
       "2019-03-20   98.449997  104.9046  100.645084  63.818062  104.9046  ...   \n",
       "\n",
       "            Momentum_7_Lag_Avg_1_3_sec  Momentum_7_Lag_Std_1_3_sec  \\\n",
       "Date                                                                 \n",
       "2019-03-14                    0.419998                1.000000e-10   \n",
       "2019-03-15                    0.419998                1.000000e-10   \n",
       "2019-03-18                    0.419998                1.000000e-10   \n",
       "2019-03-19                    0.419998                1.000000e-10   \n",
       "2019-03-20                    0.419998                1.000000e-10   \n",
       "\n",
       "            Momentum_30_Lag_Avg_1_3_sec  Momentum_30_Lag_Std_1_3_sec  \\\n",
       "Date                                                                   \n",
       "2019-03-14                     2.940002                 1.000000e-10   \n",
       "2019-03-15                     2.940002                 1.000000e-10   \n",
       "2019-03-18                     2.940002                 1.000000e-10   \n",
       "2019-03-19                     2.940002                 1.000000e-10   \n",
       "2019-03-20                     2.940002                 1.000000e-10   \n",
       "\n",
       "            Momentum_50_Lag_Avg_1_3_sec  Momentum_50_Lag_Std_1_3_sec  \\\n",
       "Date                                                                   \n",
       "2019-03-14                    12.889999                 1.000000e-10   \n",
       "2019-03-15                    12.889999                 1.000000e-10   \n",
       "2019-03-18                    12.889999                 1.000000e-10   \n",
       "2019-03-19                    12.889999                 1.000000e-10   \n",
       "2019-03-20                    12.889999                 1.000000e-10   \n",
       "\n",
       "            OBV_Lag_Avg_1_3_sec  OBV_Lag_Std_1_3_sec  Diff_Close_EMA_sec  \\\n",
       "Date                                                                       \n",
       "2019-03-14         1.000000e-10         1.611779e+06        1.000000e-10   \n",
       "2019-03-15         1.000000e-10         1.611779e+06        1.000000e-10   \n",
       "2019-03-18         1.000000e-10         1.611779e+06        1.000000e-10   \n",
       "2019-03-19         1.000000e-10         2.100176e+06        1.000000e-10   \n",
       "2019-03-20         1.000000e-10         9.614140e+05        1.000000e-10   \n",
       "\n",
       "            Ratio_Close_EMA_sec  \n",
       "Date                             \n",
       "2019-03-14             1.000000  \n",
       "2019-03-15             0.994006  \n",
       "2019-03-18             0.965934  \n",
       "2019-03-19             0.972002  \n",
       "2019-03-20             0.981767  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's rename our two columns.\n",
    "core_stock_data.rename(columns = {'Ticker' : 'core_ticker'}, inplace = True)\n",
    "sec_stock_data.rename(columns = {'ticker' : 'sec_ticker'}, inplace = True)\n",
    "\n",
    "# Now let's make a variable that we can use for our cointegrated pairs\n",
    "cointegrated_stocks = ['AZN', 'ROL', 'VTRS'] # To be edited for each core stock's results.\n",
    "\n",
    "sec_stock_data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The changes have been applied, and we are now ready to start preparing and building our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now set up our X,y for the model.  Similar to how we did for the Linear Regression models in this project we will eschew the traditional train_test_split methodology in favor of a more straightforward approach that works well with time series data.  We will also be using our cointegrated stocks here along with their data in the sec_stock_data dataframe and leaving the rest behind, and then adding the exo_data (exogenous data) to the mix to see what may be added here to the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Decision Tree for AZN\n",
      "X_train shape: (1092, 24), X_test shape: (273, 24)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 12.258862377378506\n",
      "Test RMSE: 35.88596193010603\n",
      "Train MAPE: 7.795567219125483\n",
      "Test MAPE: 15.96720028871993\n",
      "Train R2: 0.9157503154716641\n",
      "Test R2: -3.979866391105107\n",
      "Train Adj R2: 0.9138552897653097\n",
      "Test Adj R2: -4.46178894508302\n",
      "Cross-Validation RMSE scores: [59.18928626 35.87490651 20.16587157 20.82781133 35.88596193]\n",
      "Mean Cross-Validation RMSE: 34.388767517979\n",
      "Feature Importances:\n",
      "                        Feature  Importance\n",
      "8                       OBV_sec    0.868730\n",
      "12                OBV_sec_Lag_3    0.046344\n",
      "13        Momentum_50_sec_Lag_1    0.028657\n",
      "6           EMA_Lag_Std_1_3_sec    0.017529\n",
      "16          BBU_Lag_Std_1_3_sec    0.010891\n",
      "14        Momentum_50_sec_Lag_3    0.008035\n",
      "17          BBL_Lag_Std_1_3_sec    0.006318\n",
      "11  MACD_Signal_Lag_Std_1_3_sec    0.005780\n",
      "7     MACD_Hist_Lag_Std_1_3_sec    0.004267\n",
      "1          Open_Lag_Std_1_3_sec    0.002105\n",
      "15           Diff_Close_EMA_sec    0.000985\n",
      "10       ATR_14_Lag_Std_1_3_sec    0.000253\n",
      "0    Momentum_1_Lag_Std_1_3_sec    0.000107\n",
      "9          MACD_Lag_Std_1_3_sec    0.000000\n",
      "5          High_Lag_Std_1_3_sec    0.000000\n",
      "4           Low_Lag_Std_1_3_sec    0.000000\n",
      "3   Momentum_50_Lag_Std_1_3_sec    0.000000\n",
      "2   Momentum_30_Lag_Std_1_3_sec    0.000000\n",
      "18       Volume_Lag_Std_1_3_sec    0.000000\n",
      "19         Momentum_7_sec_Lag_3    0.000000\n",
      "20   Momentum_3_Lag_Avg_1_3_sec    0.000000\n",
      "21          MACD_Hist_sec_Lag_3    0.000000\n",
      "22         Momentum_1_sec_Lag_2    0.000000\n",
      "23         Momentum_1_sec_Lag_3    0.000000\n",
      "Building Decision Tree for ROL\n",
      "X_train shape: (1092, 24), X_test shape: (273, 24)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 19.87837132938976\n",
      "Test RMSE: 58.385469948376375\n",
      "Train MAPE: 13.945635603043788\n",
      "Test MAPE: 28.332339819473862\n",
      "Train R2: 0.7784715329276035\n",
      "Test R2: -12.181901653837103\n",
      "Train Adj R2: 0.7734886995539039\n",
      "Test Adj R2: -13.45756955582134\n",
      "Cross-Validation RMSE scores: [83.96195222 50.53783101 34.65146183 27.61233013 58.38546995]\n",
      "Mean Cross-Validation RMSE: 51.02980902767848\n",
      "Feature Importances:\n",
      "                        Feature  Importance\n",
      "12                OBV_sec_Lag_3    0.950943\n",
      "8                       OBV_sec    0.049057\n",
      "13        Momentum_50_sec_Lag_1    0.000000\n",
      "22         Momentum_1_sec_Lag_2    0.000000\n",
      "21          MACD_Hist_sec_Lag_3    0.000000\n",
      "20   Momentum_3_Lag_Avg_1_3_sec    0.000000\n",
      "19         Momentum_7_sec_Lag_3    0.000000\n",
      "18       Volume_Lag_Std_1_3_sec    0.000000\n",
      "17          BBL_Lag_Std_1_3_sec    0.000000\n",
      "16          BBU_Lag_Std_1_3_sec    0.000000\n",
      "15           Diff_Close_EMA_sec    0.000000\n",
      "14        Momentum_50_sec_Lag_3    0.000000\n",
      "0    Momentum_1_Lag_Std_1_3_sec    0.000000\n",
      "1          Open_Lag_Std_1_3_sec    0.000000\n",
      "11  MACD_Signal_Lag_Std_1_3_sec    0.000000\n",
      "10       ATR_14_Lag_Std_1_3_sec    0.000000\n",
      "9          MACD_Lag_Std_1_3_sec    0.000000\n",
      "7     MACD_Hist_Lag_Std_1_3_sec    0.000000\n",
      "6           EMA_Lag_Std_1_3_sec    0.000000\n",
      "5          High_Lag_Std_1_3_sec    0.000000\n",
      "4           Low_Lag_Std_1_3_sec    0.000000\n",
      "3   Momentum_50_Lag_Std_1_3_sec    0.000000\n",
      "2   Momentum_30_Lag_Std_1_3_sec    0.000000\n",
      "23         Momentum_1_sec_Lag_3    0.000000\n",
      "Building Decision Tree for VTRS\n",
      "X_train shape: (1092, 24), X_test shape: (273, 24)\n",
      "y_train shape(1092,), y_test shape: (273,)\n",
      "Train RMSE: 24.43853373250738\n",
      "Test RMSE: 45.877585363631795\n",
      "Train MAPE: 18.318805424149073\n",
      "Test MAPE: 21.38194328665567\n",
      "Train R2: 0.6651747241923831\n",
      "Test R2: -7.1389730546136665\n",
      "Train Adj R2: 0.6576435089914621\n",
      "Test Adj R2: -7.926615608285957\n",
      "Cross-Validation RMSE scores: [70.18720409 55.36345808 37.56050198 21.95621718 45.87758536]\n",
      "Mean Cross-Validation RMSE: 46.18899333855459\n",
      "Feature Importances:\n",
      "                        Feature  Importance\n",
      "0    Momentum_1_Lag_Std_1_3_sec    0.334847\n",
      "8                       OBV_sec    0.200114\n",
      "3   Momentum_50_Lag_Std_1_3_sec    0.134437\n",
      "12                OBV_sec_Lag_3    0.072378\n",
      "11  MACD_Signal_Lag_Std_1_3_sec    0.048662\n",
      "13        Momentum_50_sec_Lag_1    0.046388\n",
      "6           EMA_Lag_Std_1_3_sec    0.041897\n",
      "7     MACD_Hist_Lag_Std_1_3_sec    0.033433\n",
      "18       Volume_Lag_Std_1_3_sec    0.026448\n",
      "21          MACD_Hist_sec_Lag_3    0.025199\n",
      "5          High_Lag_Std_1_3_sec    0.019074\n",
      "17          BBL_Lag_Std_1_3_sec    0.008910\n",
      "16          BBU_Lag_Std_1_3_sec    0.008213\n",
      "4           Low_Lag_Std_1_3_sec    0.000000\n",
      "22         Momentum_1_sec_Lag_2    0.000000\n",
      "20   Momentum_3_Lag_Avg_1_3_sec    0.000000\n",
      "19         Momentum_7_sec_Lag_3    0.000000\n",
      "14        Momentum_50_sec_Lag_3    0.000000\n",
      "15           Diff_Close_EMA_sec    0.000000\n",
      "1          Open_Lag_Std_1_3_sec    0.000000\n",
      "10       ATR_14_Lag_Std_1_3_sec    0.000000\n",
      "9          MACD_Lag_Std_1_3_sec    0.000000\n",
      "2   Momentum_30_Lag_Std_1_3_sec    0.000000\n",
      "23         Momentum_1_sec_Lag_3    0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's set up our Decision Tree model and get it ready.\n",
    "# We have to do some prep work here.  We have our core data and cointegrated pairs that we have to make sure make it to X.\n",
    "\n",
    "# To hopefully improve our net metric scores and reduce noise in the model we will utilize our optimized features we acquired using our VIF and correlation matrix analysis in a previous notebook, that was also use to generate our sec_stock_data stocks.\n",
    "optimized_features = ['Momentum_1_Lag_Std_1_3_sec', 'Open_Lag_Std_1_3_sec', 'Momentum_30_Lag_Std_1_3_sec', 'Momentum_50_Lag_Std_1_3_sec',\n",
    "                'Low_Lag_Std_1_3_sec', 'High_Lag_Std_1_3_sec', 'EMA_Lag_Std_1_3_sec', 'MACD_Hist_Lag_Std_1_3_sec', 'OBV_sec', 'MACD_Lag_Std_1_3_sec',\n",
    "                'ATR_14_Lag_Std_1_3_sec', 'MACD_Signal_Lag_Std_1_3_sec', 'OBV_sec_Lag_3', 'Momentum_50_sec_Lag_1', 'Momentum_50_sec_Lag_3', 'Diff_Close_EMA_sec',\n",
    "                'BBU_Lag_Std_1_3_sec', 'BBL_Lag_Std_1_3_sec', 'Volume_Lag_Std_1_3_sec', 'Momentum_7_sec_Lag_3', 'Momentum_3_Lag_Avg_1_3_sec', 'MACD_Hist_sec_Lag_3',\n",
    "                'Momentum_1_sec_Lag_2', 'Momentum_1_sec_Lag_3']\n",
    "# Let's first get the cointegrated stocks as a variable so we can use to grab their data from sec_stock_data.\n",
    "cointegrated_stocks = ['AZN', 'ROL', 'VTRS']\n",
    "\n",
    "# Loop over each cointegrated stock to build a separate tree\n",
    "for stock in cointegrated_stocks:\n",
    "    print(f\"Building Decision Tree for {stock}\")\n",
    "\n",
    "    # Let's filter our sec_stock_data for rows where the ticker is included in our cointegrated_tickers so we can grab that data together.\n",
    "    sec_stock_filtered = sec_stock_data[sec_stock_data['sec_ticker'] == stock]\n",
    "    \n",
    "    # Setting up X with the cointegrated stock data that has the excluded features.\n",
    "    X = sec_stock_filtered[optimized_features]\n",
    "    \n",
    "    # Ensure all datatypes for our X are numeric\n",
    "    X = X.apply(pd.to_numeric, errors = 'coerce')\n",
    "    y = aapl_data_orig['Close_core'] # Our target\n",
    "    \n",
    "    \n",
    "\n",
    "    # Not done yet, we are going to use split_index for Time Series on our X and y.\n",
    "    split_index = int(len(X) * 0.8)\n",
    "\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "    \n",
    "    # Let's check shapes of X and y to make sure\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape{y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    # Set up the Decision Tree model itself.\n",
    "    dec_tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    # Set up the parameter grid for tuning if needed.\n",
    "    param_grid = {\n",
    "        'max_depth' : [2, 3, 5],\n",
    "        'min_samples_split' : [10, 20],\n",
    "        'min_samples_leaf' : [5, 10]\n",
    "    }\n",
    "\n",
    "    # Use GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(dec_tree, param_grid, cv = 3, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "    # Fit the model.\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_tree = grid_search.best_estimator_\n",
    "\n",
    "    best_tree.fit(X_train, y_train)\n",
    "\n",
    "    # Now we can make predictions\n",
    "    y_train_pred = best_tree.predict(X_train)\n",
    "    y_test_pred = best_tree.predict(X_test)\n",
    "    \n",
    "    # Let's calculate the RMSE first.\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    # Let's now calculate the MAPE.\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "    mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "    # Now for the R2 scores.\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Lastly the Adjusted R2 score.\n",
    "    def adjusted_r2(r2, X):\n",
    "        n = len(X)\n",
    "        p = X.shape[1]\n",
    "        return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "    adj_r2_train = adjusted_r2(r2_train, X_train)\n",
    "    adj_r2_test = adjusted_r2(r2_test, X_test)\n",
    "\n",
    "    print(f\"Train RMSE:\", rmse_train)\n",
    "    print(f\"Test RMSE:\", rmse_test)\n",
    "\n",
    "    print(f\"Train MAPE:\", mape_train)\n",
    "    print(f\"Test MAPE:\", mape_test)\n",
    "\n",
    "    print(f\"Train R2:\", r2_train)\n",
    "    print(f\"Test R2:\", r2_test)\n",
    "\n",
    "    print(f\"Train Adj R2:\", adj_r2_train)\n",
    "    print(f\"Test Adj R2:\", adj_r2_test)\n",
    "    \n",
    "    # Now finally adding cross_validation to evaluate model stability\n",
    "    cv_scores = cross_val_score(best_tree, X, y, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores)\n",
    "    \n",
    "    print(f\"Cross-Validation RMSE scores: {cv_rmse}\")\n",
    "    print(f\"Mean Cross-Validation RMSE: {cv_rmse.mean()}\")\n",
    "    \n",
    "    # We will need to adjust and measure how important our optimized features are\n",
    "    feat_importances = best_tree.feature_importances_\n",
    "    \n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Create a DataFrame to sort the importances\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature' : feature_names,\n",
    "        'Importance' : feat_importances\n",
    "    })\n",
    "    importance_df = importance_df.sort_values(by = 'Importance', ascending = False)\n",
    "    \n",
    "    # Print and show the feature importances\n",
    "    print(\"Feature Importances:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot metrics obs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at a few plots based on our Decision Tree model.  First let's just take a look at the tree itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.tree' has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m(best_tree, feature_names \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns, filled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, rounded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.tree' has no attribute 'plot'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "\n",
    "tree.plot(best_tree, feature_names = X.columns, filled = True, rounded = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at another plot regarding feature importance to see how the model is using the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first make a quick variable for our feature importances to use in the plot.\n",
    "feature_importances = best_tree.feature_importances_\n",
    "\n",
    "# make these into a dataframe so they are visualized better\n",
    "feature_df = pd.DataFrame({'Feature' : X.columns, 'Importance' : feature_importances})\n",
    "\n",
    "# Sort these features by their importance.\n",
    "feature_df = feature_df.sort_values(by = 'Importance', ascending=False)\n",
    "\n",
    "# Now we can plot and see how it looks.\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = feature_df)\n",
    "plt.title('Feature Importance in Decision Tree Model of AAPL Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot observation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One more, let's look at an Actual vs Predicted for our values for the AAPL data from our Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(y_test.values, label = 'Actual Values', color = 'blue')\n",
    "plt.plot(y_test_pred, 'Predicted Values', color = 'red')\n",
    "plt.title('Actual vs Predicted Values for AAPL Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Observation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Strat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
